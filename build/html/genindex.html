


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>索引 &mdash; PyTorch master 文档</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  
  
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  
  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />
  
    <link rel="stylesheet" href="_static/css/pytorch_theme.css" type="text/css" />
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  
        <link rel="index" title="索引"
              href="#"/>
        <link rel="search" title="搜索" href="search.html"/>
    <link rel="top" title="PyTorch master 文档" href="index.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html">
          

          
            
            <img src="_static/pytorch-logo-dark.svg" class="logo" />
          
          </a>

          
            
            
              <div class="version">
                master (0.3.0.post4 )
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd mechanics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#excluding-subgraphs-from-backward">Excluding subgraphs from backward</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/autograd.html#requires-grad"><code class="docutils literal"><span class="pre">requires_grad</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/autograd.html#volatile"><code class="docutils literal"><span class="pre">volatile</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#how-autograd-encodes-the-history">How autograd encodes the history</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#in-place-operations-on-variables">In-place operations on Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#in-place-correctness-checks">In-place correctness checks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">Broadcasting semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/broadcasting.html#general-semantics">General semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/broadcasting.html#in-place-semantics">In-place semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/broadcasting.html#backwards-compatibility">Backwards compatibility</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/cuda.html#memory-management">Memory management</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/cuda.html#best-practices">Best practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#device-agnostic-code">Device-agnostic code</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#use-pinned-memory-buffers">Use pinned memory buffers</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#use-nn-dataparallel-instead-of-multiprocessing">Use nn.DataParallel instead of multiprocessing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">Extending PyTorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/extending.html#extending-torch-autograd">Extending <code class="docutils literal"><span class="pre">torch.autograd</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/extending.html#extending-torch-nn">Extending <code class="docutils literal"><span class="pre">torch.nn</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/extending.html#adding-a-module">Adding a <code class="docutils literal"><span class="pre">Module</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/extending.html#writing-custom-c-extensions">Writing custom C extensions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">Multiprocessing best practices</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/multiprocessing.html#sharing-cuda-tensors">Sharing CUDA tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/multiprocessing.html#best-practices-and-tips">Best practices and tips</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/multiprocessing.html#avoiding-and-fighting-deadlocks">Avoiding and fighting deadlocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/multiprocessing.html#reuse-buffers-passed-through-a-queue">Reuse buffers passed through a Queue</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/multiprocessing.html#asynchronous-multiprocess-training-e-g-hogwild">Asynchronous multiprocess training (e.g. Hogwild)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="notes/multiprocessing.html#hogwild">Hogwild</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">Serialization semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/serialization.html#best-practices">Best practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/serialization.html#recommended-approach-for-saving-a-model">Recommended approach for saving a model</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="torch.html#tensors">Tensors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torch.html#creation-ops">Creation Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#indexing-slicing-joining-mutating-ops">Indexing, Slicing, Joining, Mutating Ops</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#random-sampling">Random sampling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torch.html#in-place-random-sampling">In-place random sampling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#serialization">Serialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#parallelism">Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#math-operations">Math operations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torch.html#pointwise-ops">Pointwise Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#reduction-ops">Reduction Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#comparison-ops">Comparison Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#other-operations">Other Operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#blas-and-lapack-operations">BLAS and LAPACK Operations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a><ul>
<li class="toctree-l2"><a class="reference internal" href="nn.html#parameters">Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#containers">Containers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#module"><span class="hidden-section">Module</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#sequential"><span class="hidden-section">Sequential</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#modulelist"><span class="hidden-section">ModuleList</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#parameterlist"><span class="hidden-section">ParameterList</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#convolution-layers">Convolution Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv1d"><span class="hidden-section">Conv1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv2d"><span class="hidden-section">Conv2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv3d"><span class="hidden-section">Conv3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#convtranspose1d"><span class="hidden-section">ConvTranspose1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#convtranspose2d"><span class="hidden-section">ConvTranspose2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#convtranspose3d"><span class="hidden-section">ConvTranspose3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#pooling-layers">Pooling Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxpool1d"><span class="hidden-section">MaxPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxpool2d"><span class="hidden-section">MaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxpool3d"><span class="hidden-section">MaxPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxunpool1d"><span class="hidden-section">MaxUnpool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxunpool2d"><span class="hidden-section">MaxUnpool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxunpool3d"><span class="hidden-section">MaxUnpool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avgpool1d"><span class="hidden-section">AvgPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avgpool2d"><span class="hidden-section">AvgPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avgpool3d"><span class="hidden-section">AvgPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#fractionalmaxpool2d"><span class="hidden-section">FractionalMaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#lppool2d"><span class="hidden-section">LPPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptivemaxpool1d"><span class="hidden-section">AdaptiveMaxPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptivemaxpool2d"><span class="hidden-section">AdaptiveMaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptivemaxpool3d"><span class="hidden-section">AdaptiveMaxPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptiveavgpool1d"><span class="hidden-section">AdaptiveAvgPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptiveavgpool2d"><span class="hidden-section">AdaptiveAvgPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptiveavgpool3d"><span class="hidden-section">AdaptiveAvgPool3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#padding-layers">Padding Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#reflectionpad2d"><span class="hidden-section">ReflectionPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#replicationpad2d"><span class="hidden-section">ReplicationPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#replicationpad3d"><span class="hidden-section">ReplicationPad3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#zeropad2d"><span class="hidden-section">ZeroPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#constantpad2d"><span class="hidden-section">ConstantPad2d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#non-linear-activations">Non-linear Activations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#relu"><span class="hidden-section">ReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#relu6"><span class="hidden-section">ReLU6</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#elu"><span class="hidden-section">ELU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#selu"><span class="hidden-section">SELU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#prelu"><span class="hidden-section">PReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#leakyrelu"><span class="hidden-section">LeakyReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#threshold"><span class="hidden-section">Threshold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#hardtanh"><span class="hidden-section">Hardtanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#sigmoid"><span class="hidden-section">Sigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#tanh"><span class="hidden-section">Tanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#logsigmoid"><span class="hidden-section">LogSigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softplus"><span class="hidden-section">Softplus</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softshrink"><span class="hidden-section">Softshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softsign"><span class="hidden-section">Softsign</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#tanhshrink"><span class="hidden-section">Tanhshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softmin"><span class="hidden-section">Softmin</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softmax"><span class="hidden-section">Softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softmax2d"><span class="hidden-section">Softmax2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#logsoftmax"><span class="hidden-section">LogSoftmax</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#normalization-layers">Normalization layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#batchnorm1d"><span class="hidden-section">BatchNorm1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#batchnorm2d"><span class="hidden-section">BatchNorm2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#batchnorm3d"><span class="hidden-section">BatchNorm3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#instancenorm1d"><span class="hidden-section">InstanceNorm1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#instancenorm2d"><span class="hidden-section">InstanceNorm2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#instancenorm3d"><span class="hidden-section">InstanceNorm3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#recurrent-layers">Recurrent layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#rnn"><span class="hidden-section">RNN</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#lstm"><span class="hidden-section">LSTM</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#gru"><span class="hidden-section">GRU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#rnncell"><span class="hidden-section">RNNCell</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#lstmcell"><span class="hidden-section">LSTMCell</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#grucell"><span class="hidden-section">GRUCell</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#linear-layers">Linear layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#linear"><span class="hidden-section">Linear</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#bilinear"><span class="hidden-section">Bilinear</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#dropout-layers">Dropout layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#dropout"><span class="hidden-section">Dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#dropout2d"><span class="hidden-section">Dropout2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#dropout3d"><span class="hidden-section">Dropout3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#alphadropout"><span class="hidden-section">AlphaDropout</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#sparse-layers">Sparse layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#embedding"><span class="hidden-section">Embedding</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#embeddingbag"><span class="hidden-section">EmbeddingBag</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#distance-functions">Distance functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#cosinesimilarity"><span class="hidden-section">CosineSimilarity</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pairwisedistance"><span class="hidden-section">PairwiseDistance</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#loss-functions">Loss functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#l1loss"><span class="hidden-section">L1Loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#mseloss"><span class="hidden-section">MSELoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#crossentropyloss"><span class="hidden-section">CrossEntropyLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#nllloss"><span class="hidden-section">NLLLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#poissonnllloss"><span class="hidden-section">PoissonNLLLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#nllloss2d"><span class="hidden-section">NLLLoss2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#kldivloss"><span class="hidden-section">KLDivLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#bceloss"><span class="hidden-section">BCELoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#bcewithlogitsloss"><span class="hidden-section">BCEWithLogitsLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#marginrankingloss"><span class="hidden-section">MarginRankingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#hingeembeddingloss"><span class="hidden-section">HingeEmbeddingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multilabelmarginloss"><span class="hidden-section">MultiLabelMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#smoothl1loss"><span class="hidden-section">SmoothL1Loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softmarginloss"><span class="hidden-section">SoftMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multilabelsoftmarginloss"><span class="hidden-section">MultiLabelSoftMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#cosineembeddingloss"><span class="hidden-section">CosineEmbeddingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multimarginloss"><span class="hidden-section">MultiMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#tripletmarginloss"><span class="hidden-section">TripletMarginLoss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#vision-layers">Vision layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pixelshuffle"><span class="hidden-section">PixelShuffle</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#upsample"><span class="hidden-section">Upsample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#upsamplingnearest2d"><span class="hidden-section">UpsamplingNearest2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#upsamplingbilinear2d"><span class="hidden-section">UpsamplingBilinear2d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#dataparallel-layers-multi-gpu-distributed">DataParallel layers (multi-GPU, distributed)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#dataparallel"><span class="hidden-section">DataParallel</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#distributeddataparallel"><span class="hidden-section">DistributedDataParallel</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#utilities">Utilities</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#clip-grad-norm"><span class="hidden-section">clip_grad_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#weight-norm"><span class="hidden-section">weight_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#remove-weight-norm"><span class="hidden-section">remove_weight_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#packedsequence"><span class="hidden-section">PackedSequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pack-padded-sequence"><span class="hidden-section">pack_padded_sequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pad-packed-sequence"><span class="hidden-section">pad_packed_sequence</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="nn.html#torch-nn-functional">torch.nn.functional</a><ul>
<li class="toctree-l2"><a class="reference internal" href="nn.html#convolution-functions">Convolution functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id14"><span class="hidden-section">conv1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id15"><span class="hidden-section">conv2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id16"><span class="hidden-section">conv3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv-transpose1d"><span class="hidden-section">conv_transpose1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv-transpose2d"><span class="hidden-section">conv_transpose2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv-transpose3d"><span class="hidden-section">conv_transpose3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#pooling-functions">Pooling functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avg-pool1d"><span class="hidden-section">avg_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avg-pool2d"><span class="hidden-section">avg_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avg-pool3d"><span class="hidden-section">avg_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-pool1d"><span class="hidden-section">max_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-pool2d"><span class="hidden-section">max_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-pool3d"><span class="hidden-section">max_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-unpool1d"><span class="hidden-section">max_unpool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-unpool2d"><span class="hidden-section">max_unpool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-unpool3d"><span class="hidden-section">max_unpool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#lp-pool2d"><span class="hidden-section">lp_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-max-pool1d"><span class="hidden-section">adaptive_max_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-max-pool2d"><span class="hidden-section">adaptive_max_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-max-pool3d"><span class="hidden-section">adaptive_max_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-avg-pool1d"><span class="hidden-section">adaptive_avg_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-avg-pool2d"><span class="hidden-section">adaptive_avg_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-avg-pool3d"><span class="hidden-section">adaptive_avg_pool3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#non-linear-activation-functions">Non-linear activation functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id17"><span class="hidden-section">threshold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id18"><span class="hidden-section">relu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id19"><span class="hidden-section">hardtanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id20"><span class="hidden-section">relu6</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id21"><span class="hidden-section">elu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id22"><span class="hidden-section">selu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#leaky-relu"><span class="hidden-section">leaky_relu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id23"><span class="hidden-section">prelu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#rrelu"><span class="hidden-section">rrelu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#glu"><span class="hidden-section">glu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id24"><span class="hidden-section">logsigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#hardshrink"><span class="hidden-section">hardshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id25"><span class="hidden-section">tanhshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id26"><span class="hidden-section">softsign</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id27"><span class="hidden-section">softplus</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id28"><span class="hidden-section">softmin</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id29"><span class="hidden-section">softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id30"><span class="hidden-section">softshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#log-softmax"><span class="hidden-section">log_softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id31"><span class="hidden-section">tanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id32"><span class="hidden-section">sigmoid</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#normalization-functions">Normalization functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#batch-norm"><span class="hidden-section">batch_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#normalize"><span class="hidden-section">normalize</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#linear-functions">Linear functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id33"><span class="hidden-section">linear</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#dropout-functions">Dropout functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id34"><span class="hidden-section">dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#alpha-dropout"><span class="hidden-section">alpha_dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id35"><span class="hidden-section">dropout2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id36"><span class="hidden-section">dropout3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#id37">Distance functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pairwise-distance"><span class="hidden-section">pairwise_distance</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#cosine-similarity"><span class="hidden-section">cosine_similarity</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#id38">Loss functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#binary-cross-entropy"><span class="hidden-section">binary_cross_entropy</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#poisson-nll-loss"><span class="hidden-section">poisson_nll_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#cosine-embedding-loss"><span class="hidden-section">cosine_embedding_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#cross-entropy"><span class="hidden-section">cross_entropy</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#hinge-embedding-loss"><span class="hidden-section">hinge_embedding_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#kl-div"><span class="hidden-section">kl_div</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#l1-loss"><span class="hidden-section">l1_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#mse-loss"><span class="hidden-section">mse_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#margin-ranking-loss"><span class="hidden-section">margin_ranking_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multilabel-margin-loss"><span class="hidden-section">multilabel_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multilabel-soft-margin-loss"><span class="hidden-section">multilabel_soft_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multi-margin-loss"><span class="hidden-section">multi_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#nll-loss"><span class="hidden-section">nll_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#binary-cross-entropy-with-logits"><span class="hidden-section">binary_cross_entropy_with_logits</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#smooth-l1-loss"><span class="hidden-section">smooth_l1_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#soft-margin-loss"><span class="hidden-section">soft_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#triplet-margin-loss"><span class="hidden-section">triplet_margin_loss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#vision-functions">Vision functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pixel-shuffle"><span class="hidden-section">pixel_shuffle</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pad"><span class="hidden-section">pad</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id40"><span class="hidden-section">upsample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#upsample-nearest"><span class="hidden-section">upsample_nearest</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#upsample-bilinear"><span class="hidden-section">upsample_bilinear</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#grid-sample"><span class="hidden-section">grid_sample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#affine-grid"><span class="hidden-section">affine_grid</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="nn.html#torch-nn-init">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a><ul>
<li class="toctree-l2"><a class="reference internal" href="optim.html#how-to-use-an-optimizer">How to use an optimizer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="optim.html#constructing-it">Constructing it</a></li>
<li class="toctree-l3"><a class="reference internal" href="optim.html#per-parameter-options">Per-parameter options</a></li>
<li class="toctree-l3"><a class="reference internal" href="optim.html#taking-an-optimization-step">Taking an optimization step</a><ul>
<li class="toctree-l4"><a class="reference internal" href="optim.html#optimizer-step"><code class="docutils literal"><span class="pre">optimizer.step()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="optim.html#optimizer-step-closure"><code class="docutils literal"><span class="pre">optimizer.step(closure)</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="optim.html#algorithms">Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="optim.html#how-to-adjust-learning-rate">How to adjust Learning Rate</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a><ul>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#variable">Variable</a><ul>
<li class="toctree-l3"><a class="reference internal" href="autograd.html#api-compatibility">API compatibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="autograd.html#in-place-operations-on-variables">In-place operations on Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="autograd.html#in-place-correctness-checks">In-place correctness checks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#function"><span class="hidden-section">Function</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#profiler">Profiler</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#distribution"><span class="hidden-section">Distribution</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#bernoulli"><span class="hidden-section">Bernoulli</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#categorical"><span class="hidden-section">Categorical</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#normal"><span class="hidden-section">Normal</span></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="multiprocessing.html">torch.multiprocessing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="multiprocessing.html#strategy-management">Strategy management</a></li>
<li class="toctree-l2"><a class="reference internal" href="multiprocessing.html#sharing-cuda-tensors">Sharing CUDA tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="multiprocessing.html#sharing-strategies">Sharing strategies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="multiprocessing.html#file-descriptor-file-descriptor">File descriptor - <code class="docutils literal"><span class="pre">file_descriptor</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="multiprocessing.html#file-system-file-system">File system - <code class="docutils literal"><span class="pre">file_system</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a><ul>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#basics">Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#initialization">Initialization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="distributed.html#tcp-initialization">TCP initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed.html#shared-file-system-initialization">Shared file-system initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed.html#environment-variable-initialization">Environment variable initialization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#groups">Groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#point-to-point-communication">Point-to-point communication</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#collective-functions">Collective functions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="legacy.html">torch.legacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a><ul>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#random-number-generator">Random Number Generator</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#communication-collectives">Communication collectives</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#streams-and-events">Streams and events</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#memory-management">Memory management</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#nvidia-tools-extension-nvtx">NVIDIA Tools Extension (NVTX)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ffi.html">torch.utils.ffi</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a><ul>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#example-end-to-end-alexnet-from-pytorch-to-caffe2">Example: End-to-end AlexNet from PyTorch to Caffe2</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#limitations">Limitations</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#supported-operators">Supported operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#functions">Functions</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">torchvision Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="torchvision/index.html">torchvision</a><ul>
<li class="toctree-l2"><a class="reference internal" href="torchvision/datasets.html">torchvision.datasets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#mnist">MNIST</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#fashion-mnist">Fashion-MNIST</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#coco">COCO</a><ul>
<li class="toctree-l4"><a class="reference internal" href="torchvision/datasets.html#captions">Captions</a></li>
<li class="toctree-l4"><a class="reference internal" href="torchvision/datasets.html#detection">Detection</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#lsun">LSUN</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#imagefolder">ImageFolder</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#imagenet-12">Imagenet-12</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#cifar">CIFAR</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#stl10">STL10</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#svhn">SVHN</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#phototour">PhotoTour</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/models.html">torchvision.models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id1">Alexnet</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id2">VGG</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id3">ResNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id4">SqueezeNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id5">DenseNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#inception-v3">Inception v3</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/transforms.html">torchvision.transforms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#transforms-on-pil-image">Transforms on PIL Image</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#transforms-on-torch-tensor">Transforms on torch.*Tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#conversion-transforms">Conversion Transforms</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#generic-transforms">Generic Transforms</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/utils.html">torchvision.utils</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">PyTorch</a>
        
      </nav>


      
      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>索引</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            

<h1 id="index">索引</h1>

<div class="genindex-jumpbox">
 <a href="#_"><strong>_</strong></a>
 | <a href="#A"><strong>A</strong></a>
 | <a href="#B"><strong>B</strong></a>
 | <a href="#C"><strong>C</strong></a>
 | <a href="#D"><strong>D</strong></a>
 | <a href="#E"><strong>E</strong></a>
 | <a href="#F"><strong>F</strong></a>
 | <a href="#G"><strong>G</strong></a>
 | <a href="#H"><strong>H</strong></a>
 | <a href="#I"><strong>I</strong></a>
 | <a href="#K"><strong>K</strong></a>
 | <a href="#L"><strong>L</strong></a>
 | <a href="#M"><strong>M</strong></a>
 | <a href="#N"><strong>N</strong></a>
 | <a href="#O"><strong>O</strong></a>
 | <a href="#P"><strong>P</strong></a>
 | <a href="#Q"><strong>Q</strong></a>
 | <a href="#R"><strong>R</strong></a>
 | <a href="#S"><strong>S</strong></a>
 | <a href="#T"><strong>T</strong></a>
 | <a href="#U"><strong>U</strong></a>
 | <a href="#V"><strong>V</strong></a>
 | <a href="#W"><strong>W</strong></a>
 | <a href="#X"><strong>X</strong></a>
 | <a href="#Z"><strong>Z</strong></a>
 
</div>
<h2 id="_">_</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchvision/transforms.html#torchvision.transforms.Normalize.__call__">__call__() (torchvision.transforms.Normalize 方法)</a>

      <ul>
        <li><a href="torchvision/transforms.html#torchvision.transforms.ToPILImage.__call__">(torchvision.transforms.ToPILImage 方法)</a>
</li>
        <li><a href="torchvision/transforms.html#torchvision.transforms.ToTensor.__call__">(torchvision.transforms.ToTensor 方法)</a>
</li>
      </ul></li>
      <li><a href="torchvision/datasets.html#torchvision.datasets.CIFAR10.__getitem__">__getitem__() (torchvision.datasets.CIFAR10 方法)</a>

      <ul>
        <li><a href="torchvision/datasets.html#torchvision.datasets.CocoCaptions.__getitem__">(torchvision.datasets.CocoCaptions 方法)</a>
</li>
        <li><a href="torchvision/datasets.html#torchvision.datasets.CocoDetection.__getitem__">(torchvision.datasets.CocoDetection 方法)</a>
</li>
        <li><a href="torchvision/datasets.html#torchvision.datasets.ImageFolder.__getitem__">(torchvision.datasets.ImageFolder 方法)</a>
</li>
        <li><a href="torchvision/datasets.html#torchvision.datasets.LSUN.__getitem__">(torchvision.datasets.LSUN 方法)</a>
</li>
        <li><a href="torchvision/datasets.html#torchvision.datasets.PhotoTour.__getitem__">(torchvision.datasets.PhotoTour 方法)</a>
</li>
        <li><a href="torchvision/datasets.html#torchvision.datasets.STL10.__getitem__">(torchvision.datasets.STL10 方法)</a>
</li>
        <li><a href="torchvision/datasets.html#torchvision.datasets.SVHN.__getitem__">(torchvision.datasets.SVHN 方法)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="sparse.html#torch.sparse.FloatTensor._indices">_indices() (torch.sparse.FloatTensor 方法)</a>
</li>
      <li><a href="sparse.html#torch.sparse.FloatTensor._nnz">_nnz() (torch.sparse.FloatTensor 方法)</a>
</li>
      <li><a href="sparse.html#torch.sparse.FloatTensor._values">_values() (torch.sparse.FloatTensor 方法)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="A">A</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="tensors.html#torch.Tensor.abs">abs() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.abs">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.abs_">abs_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.acos">acos() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.acos">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.acos_">acos_() (torch.Tensor 方法)</a>
</li>
      <li><a href="optim.html#torch.optim.Adadelta">Adadelta (torch.optim 中的类)</a>
</li>
      <li><a href="optim.html#torch.optim.Adagrad">Adagrad (torch.optim 中的类)</a>
</li>
      <li><a href="optim.html#torch.optim.Adam">Adam (torch.optim 中的类)</a>
</li>
      <li><a href="optim.html#torch.optim.Adamax">Adamax (torch.optim 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.adaptive_avg_pool1d">adaptive_avg_pool1d() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.adaptive_avg_pool2d">adaptive_avg_pool2d() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.adaptive_avg_pool3d">adaptive_avg_pool3d() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.adaptive_max_pool1d">adaptive_max_pool1d() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.adaptive_max_pool2d">adaptive_max_pool2d() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.adaptive_max_pool3d">adaptive_max_pool3d() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.AdaptiveAvgPool1d">AdaptiveAvgPool1d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.AdaptiveAvgPool2d">AdaptiveAvgPool2d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.AdaptiveAvgPool3d">AdaptiveAvgPool3d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.AdaptiveMaxPool1d">AdaptiveMaxPool1d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.AdaptiveMaxPool2d">AdaptiveMaxPool2d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.AdaptiveMaxPool3d">AdaptiveMaxPool3d (torch.nn 中的类)</a>
</li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.add">add() (torch.sparse.FloatTensor 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.add">(torch.Tensor 方法)</a>
</li>
        <li><a href="torch.html#torch.add">(在 torch 模块中)</a>, <a href="torch.html#torch.add">[1]</a>, <a href="torch.html#torch.add">[2]</a>
</li>
      </ul></li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.add_">add_() (torch.sparse.FloatTensor 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.add_">(torch.Tensor 方法)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.Module.add_module">add_module() (torch.nn.Module 方法)</a>
</li>
      <li><a href="optim.html#torch.optim.Optimizer.add_param_group">add_param_group() (torch.optim.Optimizer 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.addbmm">addbmm() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.addbmm">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.addbmm_">addbmm_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.addcdiv">addcdiv() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.addcdiv">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.addcdiv_">addcdiv_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.addcmul">addcmul() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.addcmul">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.addcmul_">addcmul_() (torch.Tensor 方法)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="tensors.html#torch.Tensor.addmm">addmm() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.addmm">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.addmm_">addmm_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.addmv">addmv() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.addmv">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.addmv_">addmv_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.addr">addr() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.addr">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.addr_">addr_() (torch.Tensor 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.affine_grid">affine_grid() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.alexnet">alexnet() (在 torchvision.models 模块中)</a>
</li>
      <li><a href="tensors.html#torch.ByteTensor.all">all() (torch.ByteTensor 方法)</a>
</li>
      <li><a href="distributed.html#torch.distributed.all_gather">all_gather() (在 torch.distributed 模块中)</a>
</li>
      <li><a href="distributed.html#torch.distributed.all_reduce">all_reduce() (在 torch.distributed 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.alpha_dropout">alpha_dropout() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.AlphaDropout">AlphaDropout (torch.nn 中的类)</a>
</li>
      <li><a href="tensors.html#torch.ByteTensor.any">any() (torch.ByteTensor 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.ModuleList.append">append() (torch.nn.ModuleList 方法)</a>

      <ul>
        <li><a href="nn.html#torch.nn.ParameterList.append">(torch.nn.ParameterList 方法)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.Module.apply">apply() (torch.nn.Module 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.apply_">apply_() (torch.Tensor 方法)</a>
</li>
      <li><a href="torch.html#torch.arange">arange() (在 torch 模块中)</a>
</li>
      <li><a href="optim.html#torch.optim.ASGD">ASGD (torch.optim 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.asin">asin() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.asin">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.asin_">asin_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.atan">atan() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.atan">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.atan2">atan2() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.atan2">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.atan2_">atan2_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.atan_">atan_() (torch.Tensor 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.avg_pool1d">avg_pool1d() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.avg_pool2d">avg_pool2d() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.avg_pool3d">avg_pool3d() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.AvgPool1d">AvgPool1d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.AvgPool2d">AvgPool2d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.AvgPool3d">AvgPool3d (torch.nn 中的类)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="B">B</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="autograd.html#torch.autograd.Function.backward">backward() (torch.autograd.Function 静态方法)</a>

      <ul>
        <li><a href="autograd.html#torch.autograd.Variable.backward">(torch.autograd.Variable 方法)</a>
</li>
        <li><a href="autograd.html#torch.autograd.backward">(在 torch.autograd 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.baddbmm">baddbmm() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.baddbmm">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.baddbmm_">baddbmm_() (torch.Tensor 方法)</a>
</li>
      <li><a href="distributed.html#torch.distributed.barrier">barrier() (在 torch.distributed 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.batch_norm">batch_norm() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.BatchNorm1d">BatchNorm1d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.BatchNorm2d">BatchNorm2d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.BatchNorm3d">BatchNorm3d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.BCELoss">BCELoss (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.BCEWithLogitsLoss">BCEWithLogitsLoss (torch.nn 中的类)</a>
</li>
      <li><a href="distributions.html#torch.distributions.Bernoulli">Bernoulli (torch.distributions 中的类)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="tensors.html#torch.Tensor.bernoulli">bernoulli() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.bernoulli">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.bernoulli_">bernoulli_() (torch.Tensor 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.Bilinear">Bilinear (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.binary_cross_entropy">binary_cross_entropy() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.binary_cross_entropy_with_logits">binary_cross_entropy_with_logits() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.bmm">bmm() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.bmm">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="cuda.html#torch.cuda.comm.broadcast">broadcast() (在 torch.cuda.comm 模块中)</a>

      <ul>
        <li><a href="distributed.html#torch.distributed.broadcast">(在 torch.distributed 模块中)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.btrifact">btrifact() (在 torch 模块中)</a>
</li>
      <li><a href="torch.html#torch.btrisolve">btrisolve() (在 torch 模块中)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.byte">byte() (torch.FloatStorage 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.byte">(torch.Tensor 方法)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.ByteTensor">ByteTensor (torch 中的类)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="C">C</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.init.calculate_gain">calculate_gain() (在 torch.nn.init 模块中)</a>
</li>
      <li><a href="torch.html#torch.cat">cat() (在 torch 模块中)</a>
</li>
      <li><a href="distributions.html#torch.distributions.Categorical">Categorical (torch.distributions 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.cauchy_">cauchy_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.ceil">ceil() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.ceil">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.ceil_">ceil_() (torch.Tensor 方法)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.CenterCrop">CenterCrop (torchvision.transforms 中的类)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.char">char() (torch.FloatStorage 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.char">(torch.Tensor 方法)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.Module.children">children() (torch.nn.Module 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.chunk">chunk() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.chunk">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="torchvision/datasets.html#torchvision.datasets.CIFAR10">CIFAR10 (torchvision.datasets 中的类)</a>
</li>
      <li><a href="torchvision/datasets.html#torchvision.datasets.CIFAR100">CIFAR100 (torchvision.datasets 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.clamp">clamp() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.clamp">(在 torch 模块中)</a>, <a href="torch.html#torch.clamp">[1]</a>, <a href="torch.html#torch.clamp">[2]</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.clamp_">clamp_() (torch.Tensor 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.utils.clip_grad_norm">clip_grad_norm() (在 torch.nn.utils 模块中)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.clone">clone() (torch.FloatStorage 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.clone">(torch.Tensor 方法)</a>
</li>
        <li><a href="sparse.html#torch.sparse.FloatTensor.clone">(torch.sparse.FloatTensor 方法)</a>
</li>
      </ul></li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.coalesce">coalesce() (torch.sparse.FloatTensor 方法)</a>
</li>
      <li><a href="torchvision/datasets.html#torchvision.datasets.CocoCaptions">CocoCaptions (torchvision.datasets 中的类)</a>
</li>
      <li><a href="torchvision/datasets.html#torchvision.datasets.CocoDetection">CocoDetection (torchvision.datasets 中的类)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.ColorJitter">ColorJitter (torchvision.transforms 中的类)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.Compose">Compose (torchvision.transforms 中的类)</a>
</li>
      <li><a href="data.html#torch.utils.data.ConcatDataset">ConcatDataset (torch.utils.data 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.init.constant">constant() (在 torch.nn.init 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.ConstantPad2d">ConstantPad2d (torch.nn 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.contiguous">contiguous() (torch.Tensor 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.Conv1d">Conv1d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.conv1d">conv1d() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.Conv2d">Conv2d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.conv2d">conv2d() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.Conv3d">Conv3d (torch.nn 中的类)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.functional.conv3d">conv3d() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.conv_transpose1d">conv_transpose1d() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.conv_transpose2d">conv_transpose2d() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.conv_transpose3d">conv_transpose3d() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.ConvTranspose1d">ConvTranspose1d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.ConvTranspose2d">ConvTranspose2d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.ConvTranspose3d">ConvTranspose3d (torch.nn 中的类)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.copy_">copy_() (torch.FloatStorage 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.copy_">(torch.Tensor 方法)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.cos">cos() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.cos">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.cos_">cos_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.cosh">cosh() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.cosh">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.cosh_">cosh_() (torch.Tensor 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.cosine_embedding_loss">cosine_embedding_loss() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.cosine_similarity">cosine_similarity() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.CosineEmbeddingLoss">CosineEmbeddingLoss (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.CosineSimilarity">CosineSimilarity (torch.nn 中的类)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.cpu">cpu() (torch.FloatStorage 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.cpu">(torch.Tensor 方法)</a>
</li>
        <li><a href="nn.html#torch.nn.Module.cpu">(torch.nn.Module 方法)</a>
</li>
      </ul></li>
      <li><a href="ffi.html#torch.utils.ffi.create_extension">create_extension() (在 torch.utils.ffi 模块中)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.cross">cross() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.cross">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.functional.cross_entropy">cross_entropy() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.CrossEntropyLoss">CrossEntropyLoss (torch.nn 中的类)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.cuda">cuda() (torch.FloatStorage 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.cuda">(torch.Tensor 方法)</a>
</li>
        <li><a href="nn.html#torch.nn.Module.cuda">(torch.nn.Module 方法)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.cumprod">cumprod() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.cumprod">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.cumsum">cumsum() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.cumsum">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="cuda.html#torch.cuda.current_blas_handle">current_blas_handle() (在 torch.cuda 模块中)</a>
</li>
      <li><a href="cuda.html#torch.cuda.current_device">current_device() (在 torch.cuda 模块中)</a>
</li>
      <li><a href="cuda.html#torch.cuda.current_stream">current_stream() (在 torch.cuda 模块中)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="D">D</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="storage.html#torch.FloatStorage.data_ptr">data_ptr() (torch.FloatStorage 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.data_ptr">(torch.Tensor 方法)</a>
</li>
      </ul></li>
      <li><a href="data.html#torch.utils.data.DataLoader">DataLoader (torch.utils.data 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.DataParallel">DataParallel (torch.nn 中的类)</a>
</li>
      <li><a href="data.html#torch.utils.data.Dataset">Dataset (torch.utils.data 中的类)</a>
</li>
      <li><a href="torch.html#torch.default_generator">default_generator() (在 torch 模块中)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.densenet121">densenet121() (在 torchvision.models 模块中)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.densenet161">densenet161() (在 torchvision.models 模块中)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.densenet169">densenet169() (在 torchvision.models 模块中)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.densenet201">densenet201() (在 torchvision.models 模块中)</a>
</li>
      <li><a href="autograd.html#torch.autograd.Variable.detach">detach() (torch.autograd.Variable 方法)</a>
</li>
      <li><a href="autograd.html#torch.autograd.Variable.detach_">detach_() (torch.autograd.Variable 方法)</a>
</li>
      <li><a href="cuda.html#torch.cuda.device">device (torch.cuda 中的类)</a>
</li>
      <li><a href="cuda.html#torch.cuda.device_count">device_count() (在 torch.cuda 模块中)</a>
</li>
      <li><a href="cuda.html#torch.cuda.device_ctx_manager">device_ctx_manager() (在 torch.cuda 模块中)</a>
</li>
      <li><a href="cuda.html#torch.cuda.device_of">device_of (torch.cuda 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.diag">diag() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.diag">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.dim">dim() (torch.sparse.FloatTensor 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.dim">(torch.Tensor 方法)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.init.dirac">dirac() (在 torch.nn.init 模块中)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="tensors.html#torch.Tensor.dist">dist() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.dist">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.parallel.DistributedDataParallel">DistributedDataParallel (torch.nn.parallel 中的类)</a>
</li>
      <li><a href="data.html#torch.utils.data.distributed.DistributedSampler">DistributedSampler (torch.utils.data.distributed 中的类)</a>
</li>
      <li><a href="distributions.html#torch.distributions.Distribution">Distribution (torch.distributions 中的类)</a>
</li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.div">div() (torch.sparse.FloatTensor 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.div">(torch.Tensor 方法)</a>
</li>
        <li><a href="torch.html#torch.div">(在 torch 模块中)</a>, <a href="torch.html#torch.div">[1]</a>, <a href="torch.html#torch.div">[2]</a>
</li>
      </ul></li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.div_">div_() (torch.sparse.FloatTensor 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.div_">(torch.Tensor 方法)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.dot">dot() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.dot">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="storage.html#torch.FloatStorage.double">double() (torch.FloatStorage 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.double">(torch.Tensor 方法)</a>
</li>
        <li><a href="nn.html#torch.nn.Module.double">(torch.nn.Module 方法)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.Dropout">Dropout (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.dropout">dropout() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.Dropout2d">Dropout2d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.dropout2d">dropout2d() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.Dropout3d">Dropout3d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.dropout3d">dropout3d() (在 torch.nn.functional 模块中)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="E">E</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="tensors.html#torch.Tensor.eig">eig() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.eig">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="cuda.html#torch.cuda.Event.elapsed_time">elapsed_time() (torch.cuda.Event 方法)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.element_size">element_size() (torch.FloatStorage 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.element_size">(torch.Tensor 方法)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.ELU">ELU (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.elu">elu() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.Embedding">Embedding (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.EmbeddingBag">EmbeddingBag (torch.nn 中的类)</a>
</li>
      <li><a href="autograd.html#torch.autograd.profiler.emit_nvtx">emit_nvtx (torch.autograd.profiler 中的类)</a>
</li>
      <li><a href="cuda.html#torch.cuda.empty_cache">empty_cache() (在 torch.cuda 模块中)</a>, <a href="cuda.html#torch.cuda.empty_cache">[1]</a>
</li>
      <li><a href="tensors.html#torch.Tensor.eq">eq() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.eq">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.eq_">eq_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.equal">equal() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.equal">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.erf">erf() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.erf">(在 torch 模块中)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="tensors.html#torch.Tensor.erf_">erf_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.erfinv">erfinv() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.erfinv">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.erfinv_">erfinv_() (torch.Tensor 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.eval">eval() (torch.nn.Module 方法)</a>
</li>
      <li><a href="cuda.html#torch.cuda.Event">Event (torch.cuda 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.exp">exp() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.exp">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.exp_">exp_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.expand">expand() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.expand_as">expand_as() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.exponential_">exponential_() (torch.Tensor 方法)</a>
</li>
      <li><a href="optim.html#torch.optim.lr_scheduler.ExponentialLR">ExponentialLR (torch.optim.lr_scheduler 中的类)</a>
</li>
      <li><a href="onnx.html#torch.onnx.export">export() (在 torch.onnx 模块中)</a>
</li>
      <li><a href="autograd.html#torch.autograd.profiler.profile.export_chrome_trace">export_chrome_trace() (torch.autograd.profiler.profile 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.ModuleList.extend">extend() (torch.nn.ModuleList 方法)</a>

      <ul>
        <li><a href="nn.html#torch.nn.ParameterList.extend">(torch.nn.ParameterList 方法)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.eye">eye() (在 torch 模块中)</a>

      <ul>
        <li><a href="nn.html#torch.nn.init.eye">(在 torch.nn.init 模块中)</a>
</li>
      </ul></li>
  </ul></td>
</tr></table>

<h2 id="F">F</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchvision/datasets.html#torchvision.datasets.FashionMNIST">FashionMNIST (torchvision.datasets 中的类)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.fill_">fill_() (torch.FloatStorage 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.fill_">(torch.Tensor 方法)</a>
</li>
      </ul></li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.FiveCrop">FiveCrop (torchvision.transforms 中的类)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.float">float() (torch.FloatStorage 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.float">(torch.Tensor 方法)</a>
</li>
        <li><a href="nn.html#torch.nn.Module.float">(torch.nn.Module 方法)</a>
</li>
      </ul></li>
      <li><a href="storage.html#torch.FloatStorage">FloatStorage (torch 中的类)</a>
</li>
      <li><a href="sparse.html#torch.sparse.FloatTensor">FloatTensor (torch.sparse 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.floor">floor() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.floor">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.floor_">floor_() (torch.Tensor 方法)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="tensors.html#torch.Tensor.fmod">fmod() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.fmod">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.fmod_">fmod_() (torch.Tensor 方法)</a>
</li>
      <li><a href="autograd.html#torch.autograd.Function.forward">forward() (torch.autograd.Function 静态方法)</a>

      <ul>
        <li><a href="nn.html#torch.nn.Module.forward">(torch.nn.Module 方法)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.frac">frac() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.frac">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.frac_">frac_() (torch.Tensor 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.FractionalMaxPool2d">FractionalMaxPool2d (torch.nn 中的类)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.from_buffer">from_buffer() (torch.FloatStorage 方法)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.from_file">from_file() (torch.FloatStorage 方法)</a>
</li>
      <li><a href="torch.html#torch.from_numpy">from_numpy() (在 torch 模块中)</a>
</li>
      <li><a href="autograd.html#torch.autograd.Function">Function (torch.autograd 中的类)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="G">G</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="tensors.html#torch.Tensor.gather">gather() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.gather">(在 torch 模块中)</a>
</li>
        <li><a href="cuda.html#torch.cuda.comm.gather">(在 torch.cuda.comm 模块中)</a>
</li>
        <li><a href="distributed.html#torch.distributed.gather">(在 torch.distributed 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.ge">ge() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.ge">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.ge_">ge_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.gels">gels() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.gels">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.geometric_">geometric_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.geqrf">geqrf() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.geqrf">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.ger">ger() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.ger">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.gesv">gesv() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.gesv">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="multiprocessing.html#torch.multiprocessing.get_all_sharing_strategies">get_all_sharing_strategies() (在 torch.multiprocessing 模块中)</a>
</li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.get_device">get_device() (torch.sparse.FloatTensor 方法)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="cuda.html#torch.cuda.get_device_capability">get_device_capability() (在 torch.cuda 模块中)</a>
</li>
      <li><a href="cuda.html#torch.cuda.get_device_name">get_device_name() (在 torch.cuda 模块中)</a>
</li>
      <li><a href="torchvision/index.html#torchvision.get_image_backend">get_image_backend() (在 torchvision 模块中)</a>
</li>
      <li><a href="torch.html#torch.get_num_threads">get_num_threads() (在 torch 模块中)</a>
</li>
      <li><a href="distributed.html#torch.distributed.get_rank">get_rank() (在 torch.distributed 模块中)</a>
</li>
      <li><a href="torch.html#torch.get_rng_state">get_rng_state() (在 torch 模块中)</a>

      <ul>
        <li><a href="cuda.html#torch.cuda.get_rng_state">(在 torch.cuda 模块中)</a>
</li>
      </ul></li>
      <li><a href="multiprocessing.html#torch.multiprocessing.get_sharing_strategy">get_sharing_strategy() (在 torch.multiprocessing 模块中)</a>
</li>
      <li><a href="distributed.html#torch.distributed.get_world_size">get_world_size() (在 torch.distributed 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.glu">glu() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="autograd.html#torch.autograd.grad">grad() (在 torch.autograd 模块中)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.Grayscale">Grayscale (torchvision.transforms 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.grid_sample">grid_sample() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.GRU">GRU (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.GRUCell">GRUCell (torch.nn 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.gt">gt() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.gt">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.gt_">gt_() (torch.Tensor 方法)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="H">H</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="storage.html#torch.FloatStorage.half">half() (torch.FloatStorage 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.half">(torch.Tensor 方法)</a>
</li>
        <li><a href="nn.html#torch.nn.Module.half">(torch.nn.Module 方法)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.functional.hardshrink">hardshrink() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.Hardtanh">Hardtanh (torch.nn 中的类)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.functional.hardtanh">hardtanh() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.hinge_embedding_loss">hinge_embedding_loss() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.HingeEmbeddingLoss">HingeEmbeddingLoss (torch.nn 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.histc">histc() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.histc">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.hspmm">hspmm() (torch.sparse.FloatTensor 方法)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="I">I</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchvision/datasets.html#torchvision.datasets.ImageFolder">ImageFolder (torchvision.datasets 中的类)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.inception_v3">inception_v3() (在 torchvision.models 模块中)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.index">index() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.index_add_">index_add_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.index_copy_">index_copy_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.index_fill_">index_fill_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.index_select">index_select() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.index_select">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="distributed.html#torch.distributed.init_process_group">init_process_group() (在 torch.distributed 模块中)</a>
</li>
      <li><a href="torch.html#torch.initial_seed">initial_seed() (在 torch 模块中)</a>

      <ul>
        <li><a href="cuda.html#torch.cuda.initial_seed">(在 torch.cuda 模块中)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.InstanceNorm1d">InstanceNorm1d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.InstanceNorm2d">InstanceNorm2d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.InstanceNorm3d">InstanceNorm3d (torch.nn 中的类)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.int">int() (torch.FloatStorage 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.int">(torch.Tensor 方法)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.inverse">inverse() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.inverse">(在 torch 模块中)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="cuda.html#torch.cuda.Event.ipc_handle">ipc_handle() (torch.cuda.Event 方法)</a>
</li>
      <li><a href="distributed.html#torch.distributed.irecv">irecv() (在 torch.distributed 模块中)</a>
</li>
      <li><a href="cuda.html#torch.cuda.is_available">is_available() (在 torch.cuda 模块中)</a>
</li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.is_coalesced">is_coalesced() (torch.sparse.FloatTensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.is_contiguous">is_contiguous() (torch.Tensor 方法)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.is_cuda">is_cuda (torch.FloatStorage 属性)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.is_cuda">(torch.Tensor 属性)</a>
</li>
      </ul></li>
      <li><a href="storage.html#torch.FloatStorage.is_pinned">is_pinned() (torch.FloatStorage 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.is_pinned">(torch.Tensor 方法)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.is_set_to">is_set_to() (torch.Tensor 方法)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.is_shared">is_shared() (torch.FloatStorage 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.is_signed">is_signed() (torch.Tensor 方法)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.is_sparse">is_sparse (torch.FloatStorage 属性)</a>
</li>
      <li><a href="torch.html#torch.is_storage">is_storage() (在 torch 模块中)</a>
</li>
      <li><a href="torch.html#torch.is_tensor">is_tensor() (在 torch 模块中)</a>
</li>
      <li><a href="distributed.html#torch.distributed.isend">isend() (在 torch.distributed 模块中)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="K">K</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.init.kaiming_normal">kaiming_normal() (在 torch.nn.init 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.init.kaiming_uniform">kaiming_uniform() (在 torch.nn.init 模块中)</a>
</li>
      <li><a href="autograd.html#torch.autograd.profiler.profile.key_averages">key_averages() (torch.autograd.profiler.profile 方法)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.functional.kl_div">kl_div() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.KLDivLoss">KLDivLoss (torch.nn 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.kthvalue">kthvalue() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.kthvalue">(在 torch 模块中)</a>
</li>
      </ul></li>
  </ul></td>
</tr></table>

<h2 id="L">L</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.functional.l1_loss">l1_loss() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.L1Loss">L1Loss (torch.nn 中的类)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.Lambda">Lambda (torchvision.transforms 中的类)</a>
</li>
      <li><a href="optim.html#torch.optim.lr_scheduler.LambdaLR">LambdaLR (torch.optim.lr_scheduler 中的类)</a>
</li>
      <li><a href="optim.html#torch.optim.LBFGS">LBFGS (torch.optim 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.le">le() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.le">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.le_">le_() (torch.Tensor 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.leaky_relu">leaky_relu() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.LeakyReLU">LeakyReLU (torch.nn 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.lerp">lerp() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.lerp">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.lerp_">lerp_() (torch.Tensor 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.Linear">Linear (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.linear">linear() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="torch.html#torch.linspace">linspace() (在 torch 模块中)</a>
</li>
      <li><a href="torch.html#torch.load">load() (在 torch 模块中)</a>
</li>
      <li><a href="autograd.html#torch.autograd.profiler.load_nvprof">load_nvprof() (在 torch.autograd.profiler 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.load_state_dict">load_state_dict() (torch.nn.Module 方法)</a>

      <ul>
        <li><a href="optim.html#torch.optim.Optimizer.load_state_dict">(torch.optim.Optimizer 方法)</a>
</li>
      </ul></li>
      <li><a href="model_zoo.html#torch.utils.model_zoo.load_url">load_url() (在 torch.utils.model_zoo 模块中)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.log">log() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.log">(在 torch 模块中)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="tensors.html#torch.Tensor.log1p">log1p() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.log1p">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.log1p_">log1p_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.log_">log_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.log_normal_">log_normal_() (torch.Tensor 方法)</a>
</li>
      <li><a href="distributions.html#torch.distributions.Distribution.log_prob">log_prob() (torch.distributions.Distribution 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.log_softmax">log_softmax() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.LogSigmoid">LogSigmoid (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.logsigmoid">logsigmoid() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.LogSoftmax">LogSoftmax (torch.nn 中的类)</a>
</li>
      <li><a href="torch.html#torch.logspace">logspace() (在 torch 模块中)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.long">long() (torch.FloatStorage 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.long">(torch.Tensor 方法)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.functional.lp_pool2d">lp_pool2d() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.LPPool2d">LPPool2d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.LSTM">LSTM (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.LSTMCell">LSTMCell (torch.nn 中的类)</a>
</li>
      <li><a href="torchvision/datasets.html#torchvision.datasets.LSUN">LSUN (torchvision.datasets 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.lt">lt() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.lt">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.lt_">lt_() (torch.Tensor 方法)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="M">M</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchvision/utils.html#torchvision.utils.make_grid">make_grid() (在 torchvision.utils 模块中)</a>
</li>
      <li><a href="torch.html#torch.manual_seed">manual_seed() (在 torch 模块中)</a>

      <ul>
        <li><a href="cuda.html#torch.cuda.manual_seed">(在 torch.cuda 模块中)</a>
</li>
      </ul></li>
      <li><a href="cuda.html#torch.cuda.manual_seed_all">manual_seed_all() (在 torch.cuda 模块中)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.map_">map_() (torch.Tensor 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.margin_ranking_loss">margin_ranking_loss() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.MarginRankingLoss">MarginRankingLoss (torch.nn 中的类)</a>
</li>
      <li><a href="cuda.html#torch.cuda.nvtx.mark">mark() (在 torch.cuda.nvtx 模块中)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.masked_fill_">masked_fill_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.masked_scatter_">masked_scatter_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.masked_select">masked_select() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.masked_select">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.matmul">matmul() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.matmul">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.max">max() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.max">(在 torch 模块中)</a>, <a href="torch.html#torch.max">[1]</a>, <a href="torch.html#torch.max">[2]</a>, <a href="torch.html#torch.max">[3]</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.functional.max_pool1d">max_pool1d() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.max_pool2d">max_pool2d() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.max_pool3d">max_pool3d() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.max_unpool1d">max_unpool1d() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.max_unpool2d">max_unpool2d() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.max_unpool3d">max_unpool3d() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.MaxPool1d">MaxPool1d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.MaxPool2d">MaxPool2d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.MaxPool3d">MaxPool3d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.MaxUnpool1d">MaxUnpool1d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.MaxUnpool2d">MaxUnpool2d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.MaxUnpool3d">MaxUnpool3d (torch.nn 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.mean">mean() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.mean">(在 torch 模块中)</a>, <a href="torch.html#torch.mean">[1]</a>, <a href="torch.html#torch.mean">[2]</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="tensors.html#torch.Tensor.median">median() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.median">(在 torch 模块中)</a>, <a href="torch.html#torch.median">[1]</a>, <a href="torch.html#torch.median">[2]</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.min">min() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.min">(在 torch 模块中)</a>, <a href="torch.html#torch.min">[1]</a>, <a href="torch.html#torch.min">[2]</a>, <a href="torch.html#torch.min">[3]</a>
</li>
      </ul></li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.mm">mm() (torch.sparse.FloatTensor 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.mm">(torch.Tensor 方法)</a>
</li>
        <li><a href="torch.html#torch.mm">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="torchvision/datasets.html#torchvision.datasets.MNIST">MNIST (torchvision.datasets 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.mode">mode() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.mode">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.Module">Module (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.ModuleList">ModuleList (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.modules">modules() (torch.nn.Module 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.mse_loss">mse_loss() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.MSELoss">MSELoss (torch.nn 中的类)</a>
</li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.mul">mul() (torch.sparse.FloatTensor 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.mul">(torch.Tensor 方法)</a>
</li>
        <li><a href="torch.html#torch.mul">(在 torch 模块中)</a>, <a href="torch.html#torch.mul">[1]</a>, <a href="torch.html#torch.mul">[2]</a>
</li>
      </ul></li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.mul_">mul_() (torch.sparse.FloatTensor 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.mul_">(torch.Tensor 方法)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.functional.multi_margin_loss">multi_margin_loss() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.multilabel_margin_loss">multilabel_margin_loss() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.multilabel_soft_margin_loss">multilabel_soft_margin_loss() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.MultiLabelMarginLoss">MultiLabelMarginLoss (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.MultiLabelSoftMarginLoss">MultiLabelSoftMarginLoss (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.MultiMarginLoss">MultiMarginLoss (torch.nn 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.multinomial">multinomial() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.multinomial">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="optim.html#torch.optim.lr_scheduler.MultiStepLR">MultiStepLR (torch.optim.lr_scheduler 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.mv">mv() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.mv">(在 torch 模块中)</a>
</li>
      </ul></li>
  </ul></td>
</tr></table>

<h2 id="N">N</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.Module.named_children">named_children() (torch.nn.Module 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.named_modules">named_modules() (torch.nn.Module 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.named_parameters">named_parameters() (torch.nn.Module 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.narrow">narrow() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.ndimension">ndimension() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.ne">ne() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.ne">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.ne_">ne_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.neg">neg() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.neg">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.neg_">neg_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.nelement">nelement() (torch.Tensor 方法)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.new">new() (torch.FloatStorage 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.new">(torch.Tensor 方法)</a>
</li>
      </ul></li>
      <li><a href="distributed.html#torch.distributed.new_group">new_group() (在 torch.distributed 模块中)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.functional.nll_loss">nll_loss() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.NLLLoss">NLLLoss (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.NLLLoss2d">NLLLoss2d (torch.nn 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.nonzero">nonzero() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.nonzero">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.norm">norm() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.norm">(在 torch 模块中)</a>, <a href="torch.html#torch.norm">[1]</a>, <a href="torch.html#torch.norm">[2]</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.Normal">Normal (torch.distributions 中的类)</a>
</li>
      <li><a href="torch.html#torch.normal">normal() (在 torch 模块中)</a>, <a href="torch.html#torch.normal">[1]</a>, <a href="torch.html#torch.normal">[2]</a>, <a href="torch.html#torch.normal">[3]</a>

      <ul>
        <li><a href="nn.html#torch.nn.init.normal">(在 torch.nn.init 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.normal_">normal_() (torch.Tensor 方法)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.Normalize">Normalize (torchvision.transforms 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.normalize">normalize() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.numel">numel() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.numel">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.numpy">numpy() (torch.Tensor 方法)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="O">O</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torch.html#torch.ones">ones() (在 torch 模块中)</a>
</li>
      <li><a href="torch.html#torch.ones_like">ones_like() (在 torch 模块中)</a>
</li>
      <li><a href="optim.html#torch.optim.Optimizer">Optimizer (torch.optim 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.orgqr">orgqr() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.orgqr">(在 torch 模块中)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="tensors.html#torch.Tensor.ormqr">ormqr() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.ormqr">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.init.orthogonal">orthogonal() (在 torch.nn.init 模块中)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="P">P</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.utils.rnn.pack_padded_sequence">pack_padded_sequence() (在 torch.nn.utils.rnn 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.utils.rnn.PackedSequence">PackedSequence() (在 torch.nn.utils.rnn 模块中)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.Pad">Pad (torchvision.transforms 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.pad">pad() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.utils.rnn.pad_packed_sequence">pad_packed_sequence() (在 torch.nn.utils.rnn 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.pairwise_distance">pairwise_distance() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.PairwiseDistance">PairwiseDistance (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.Parameter">Parameter (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.ParameterList">ParameterList (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.parameters">parameters() (torch.nn.Module 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.permute">permute() (torch.Tensor 方法)</a>
</li>
      <li><a href="torchvision/datasets.html#torchvision.datasets.PhotoTour">PhotoTour (torchvision.datasets 中的类)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.pin_memory">pin_memory() (torch.FloatStorage 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.pin_memory">(torch.Tensor 方法)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.functional.pixel_shuffle">pixel_shuffle() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.PixelShuffle">PixelShuffle (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.poisson_nll_loss">poisson_nll_loss() (在 torch.nn.functional 模块中)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.PoissonNLLLoss">PoissonNLLLoss (torch.nn 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.potrf">potrf() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.potrf">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.potri">potri() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.potri">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.potrs">potrs() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.potrs">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.pow">pow() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.pow">(在 torch 模块中)</a>, <a href="torch.html#torch.pow">[1]</a>, <a href="torch.html#torch.pow">[2]</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.pow_">pow_() (torch.Tensor 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.PReLU">PReLU (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.prelu">prelu() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.prod">prod() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.prod">(在 torch 模块中)</a>, <a href="torch.html#torch.prod">[1]</a>, <a href="torch.html#torch.prod">[2]</a>
</li>
      </ul></li>
      <li><a href="autograd.html#torch.autograd.profiler.profile">profile (torch.autograd.profiler 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.pstrf">pstrf() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.pstrf">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.put_">put_() (torch.Tensor 方法)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="Q">Q</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="tensors.html#torch.Tensor.qr">qr() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.qr">(在 torch 模块中)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="cuda.html#torch.cuda.Event.query">query() (torch.cuda.Event 方法)</a>

      <ul>
        <li><a href="cuda.html#torch.cuda.Stream.query">(torch.cuda.Stream 方法)</a>
</li>
      </ul></li>
  </ul></td>
</tr></table>

<h2 id="R">R</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torch.html#torch.rand">rand() (在 torch 模块中)</a>
</li>
      <li><a href="torch.html#torch.randn">randn() (在 torch 模块中)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.random_">random_() (torch.Tensor 方法)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.RandomCrop">RandomCrop (torchvision.transforms 中的类)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.RandomGrayscale">RandomGrayscale (torchvision.transforms 中的类)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.RandomHorizontalFlip">RandomHorizontalFlip (torchvision.transforms 中的类)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.RandomResizedCrop">RandomResizedCrop (torchvision.transforms 中的类)</a>
</li>
      <li><a href="data.html#torch.utils.data.sampler.RandomSampler">RandomSampler (torch.utils.data.sampler 中的类)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.RandomSizedCrop">RandomSizedCrop (torchvision.transforms 中的类)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.RandomVerticalFlip">RandomVerticalFlip (torchvision.transforms 中的类)</a>
</li>
      <li><a href="torch.html#torch.randperm">randperm() (在 torch 模块中)</a>
</li>
      <li><a href="torch.html#torch.range">range() (在 torch 模块中)</a>
</li>
      <li><a href="cuda.html#torch.cuda.nvtx.range_pop">range_pop() (在 torch.cuda.nvtx 模块中)</a>
</li>
      <li><a href="cuda.html#torch.cuda.nvtx.range_push">range_push() (在 torch.cuda.nvtx 模块中)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.reciprocal">reciprocal() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.reciprocal">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.reciprocal_">reciprocal_() (torch.Tensor 方法)</a>
</li>
      <li><a href="cuda.html#torch.cuda.Event.record">record() (torch.cuda.Event 方法)</a>
</li>
      <li><a href="cuda.html#torch.cuda.Stream.record_event">record_event() (torch.cuda.Stream 方法)</a>
</li>
      <li><a href="distributed.html#torch.distributed.recv">recv() (在 torch.distributed 模块中)</a>
</li>
      <li><a href="distributed.html#torch.distributed.reduce">reduce() (在 torch.distributed 模块中)</a>
</li>
      <li><a href="cuda.html#torch.cuda.comm.reduce_add">reduce_add() (在 torch.cuda.comm 模块中)</a>
</li>
      <li><a href="optim.html#torch.optim.lr_scheduler.ReduceLROnPlateau">ReduceLROnPlateau (torch.optim.lr_scheduler 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.ReflectionPad2d">ReflectionPad2d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.register_backward_hook">register_backward_hook() (torch.nn.Module 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.register_buffer">register_buffer() (torch.nn.Module 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.register_forward_hook">register_forward_hook() (torch.nn.Module 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.register_forward_pre_hook">register_forward_pre_hook() (torch.nn.Module 方法)</a>
</li>
      <li><a href="autograd.html#torch.autograd.Variable.register_hook">register_hook() (torch.autograd.Variable 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.register_parameter">register_parameter() (torch.nn.Module 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.ReLU">ReLU (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.relu">relu() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.ReLU6">ReLU6 (torch.nn 中的类)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.functional.relu6">relu6() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.remainder">remainder() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.remainder">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.remainder_">remainder_() (torch.Tensor 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.utils.remove_weight_norm">remove_weight_norm() (在 torch.nn.utils 模块中)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.renorm">renorm() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.renorm">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.renorm_">renorm_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.repeat">repeat() (torch.Tensor 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.ReplicationPad2d">ReplicationPad2d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.ReplicationPad3d">ReplicationPad3d (torch.nn 中的类)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.Resize">Resize (torchvision.transforms 中的类)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.resize_">resize_() (torch.FloatStorage 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.resize_">(torch.Tensor 方法)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.resize_as_">resize_as_() (torch.Tensor 方法)</a>
</li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.resizeAs_">resizeAs_() (torch.sparse.FloatTensor 方法)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.resnet101">resnet101() (在 torchvision.models 模块中)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.resnet152">resnet152() (在 torchvision.models 模块中)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.resnet18">resnet18() (在 torchvision.models 模块中)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.resnet34">resnet34() (在 torchvision.models 模块中)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.resnet50">resnet50() (在 torchvision.models 模块中)</a>
</li>
      <li><a href="autograd.html#torch.autograd.Variable.retain_grad">retain_grad() (torch.autograd.Variable 方法)</a>
</li>
      <li><a href="optim.html#torch.optim.RMSprop">RMSprop (torch.optim 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.RNN">RNN (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.RNNCell">RNNCell (torch.nn 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.round">round() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.round">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.round_">round_() (torch.Tensor 方法)</a>
</li>
      <li><a href="optim.html#torch.optim.Rprop">Rprop (torch.optim 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.rrelu">rrelu() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.rsqrt">rsqrt() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.rsqrt">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.rsqrt_">rsqrt_() (torch.Tensor 方法)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="S">S</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="distributions.html#torch.distributions.Distribution.sample">sample() (torch.distributions.Distribution 方法)</a>
</li>
      <li><a href="distributions.html#torch.distributions.Distribution.sample_n">sample_n() (torch.distributions.Distribution 方法)</a>
</li>
      <li><a href="data.html#torch.utils.data.sampler.Sampler">Sampler (torch.utils.data.sampler 中的类)</a>
</li>
      <li><a href="torch.html#torch.save">save() (在 torch 模块中)</a>
</li>
      <li><a href="torchvision/utils.html#torchvision.utils.save_image">save_image() (在 torchvision.utils 模块中)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.Scale">Scale (torchvision.transforms 中的类)</a>
</li>
      <li><a href="cuda.html#torch.cuda.comm.scatter">scatter() (在 torch.cuda.comm 模块中)</a>

      <ul>
        <li><a href="distributed.html#torch.distributed.scatter">(在 torch.distributed 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.scatter_">scatter_() (torch.Tensor 方法)</a>
</li>
      <li><a href="cuda.html#torch.cuda.seed">seed() (在 torch.cuda 模块中)</a>
</li>
      <li><a href="cuda.html#torch.cuda.seed_all">seed_all() (在 torch.cuda 模块中)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.select">select() (torch.Tensor 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.SELU">SELU (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.selu">selu() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="distributed.html#torch.distributed.send">send() (在 torch.distributed 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.Sequential">Sequential (torch.nn 中的类)</a>
</li>
      <li><a href="data.html#torch.utils.data.sampler.SequentialSampler">SequentialSampler (torch.utils.data.sampler 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.set_">set_() (torch.Tensor 方法)</a>
</li>
      <li><a href="torch.html#torch.set_default_tensor_type">set_default_tensor_type() (在 torch 模块中)</a>
</li>
      <li><a href="cuda.html#torch.cuda.set_device">set_device() (在 torch.cuda 模块中)</a>
</li>
      <li><a href="torchvision/index.html#torchvision.set_image_backend">set_image_backend() (在 torchvision 模块中)</a>
</li>
      <li><a href="torch.html#torch.set_num_threads">set_num_threads() (在 torch 模块中)</a>
</li>
      <li><a href="torch.html#torch.set_printoptions">set_printoptions() (在 torch 模块中)</a>
</li>
      <li><a href="torch.html#torch.set_rng_state">set_rng_state() (在 torch 模块中)</a>

      <ul>
        <li><a href="cuda.html#torch.cuda.set_rng_state">(在 torch.cuda 模块中)</a>
</li>
      </ul></li>
      <li><a href="multiprocessing.html#torch.multiprocessing.set_sharing_strategy">set_sharing_strategy() (在 torch.multiprocessing 模块中)</a>
</li>
      <li><a href="optim.html#torch.optim.SGD">SGD (torch.optim 中的类)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.share_memory_">share_memory_() (torch.FloatStorage 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.share_memory_">(torch.Tensor 方法)</a>
</li>
      </ul></li>
      <li><a href="storage.html#torch.FloatStorage.short">short() (torch.FloatStorage 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.short">(torch.Tensor 方法)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.Sigmoid">Sigmoid (torch.nn 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.sigmoid">sigmoid() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.sigmoid">(在 torch 模块中)</a>
</li>
        <li><a href="nn.html#torch.nn.functional.sigmoid">(在 torch.nn.functional 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.sigmoid_">sigmoid_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.sign">sign() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.sign">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.sign_">sign_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.sin">sin() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.sin">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.sin_">sin_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.sinh">sinh() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.sinh">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.sinh_">sinh_() (torch.Tensor 方法)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.size">size() (torch.FloatStorage 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.size">(torch.Tensor 方法)</a>
</li>
        <li><a href="sparse.html#torch.sparse.FloatTensor.size">(torch.sparse.FloatTensor 方法)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.functional.smooth_l1_loss">smooth_l1_loss() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.SmoothL1Loss">SmoothL1Loss (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.soft_margin_loss">soft_margin_loss() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.SoftMarginLoss">SoftMarginLoss (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.Softmax">Softmax (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.softmax">softmax() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.Softmax2d">Softmax2d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.Softmin">Softmin (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.softmin">softmin() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.Softplus">Softplus (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.softplus">softplus() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.Softshrink">Softshrink (torch.nn 中的类)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.functional.softshrink">softshrink() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.Softsign">Softsign (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.softsign">softsign() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.sort">sort() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.sort">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.spadd">spadd() (torch.sparse.FloatTensor 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.init.sparse">sparse() (在 torch.nn.init 模块中)</a>
</li>
      <li><a href="optim.html#torch.optim.SparseAdam">SparseAdam (torch.optim 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.split">split() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.split">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.spmm">spmm() (torch.sparse.FloatTensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.sqrt">sqrt() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.sqrt">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.sqrt_">sqrt_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.squeeze">squeeze() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.squeeze">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.squeeze_">squeeze_() (torch.Tensor 方法)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.squeezenet1_0">squeezenet1_0() (在 torchvision.models 模块中)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.squeezenet1_1">squeezenet1_1() (在 torchvision.models 模块中)</a>
</li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.sspaddmm">sspaddmm() (torch.sparse.FloatTensor 方法)</a>
</li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.sspmm">sspmm() (torch.sparse.FloatTensor 方法)</a>
</li>
      <li><a href="torch.html#torch.stack">stack() (在 torch 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.state_dict">state_dict() (torch.nn.Module 方法)</a>

      <ul>
        <li><a href="optim.html#torch.optim.Optimizer.state_dict">(torch.optim.Optimizer 方法)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.std">std() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.std">(在 torch 模块中)</a>, <a href="torch.html#torch.std">[1]</a>, <a href="torch.html#torch.std">[2]</a>
</li>
      </ul></li>
      <li><a href="optim.html#torch.optim.Adadelta.step">step() (torch.optim.Adadelta 方法)</a>

      <ul>
        <li><a href="optim.html#torch.optim.ASGD.step">(torch.optim.ASGD 方法)</a>
</li>
        <li><a href="optim.html#torch.optim.Adagrad.step">(torch.optim.Adagrad 方法)</a>
</li>
        <li><a href="optim.html#torch.optim.Adam.step">(torch.optim.Adam 方法)</a>
</li>
        <li><a href="optim.html#torch.optim.Adamax.step">(torch.optim.Adamax 方法)</a>
</li>
        <li><a href="optim.html#torch.optim.LBFGS.step">(torch.optim.LBFGS 方法)</a>
</li>
        <li><a href="optim.html#torch.optim.Optimizer.step">(torch.optim.Optimizer 方法)</a>
</li>
        <li><a href="optim.html#torch.optim.RMSprop.step">(torch.optim.RMSprop 方法)</a>
</li>
        <li><a href="optim.html#torch.optim.Rprop.step">(torch.optim.Rprop 方法)</a>
</li>
        <li><a href="optim.html#torch.optim.SGD.step">(torch.optim.SGD 方法)</a>
</li>
        <li><a href="optim.html#torch.optim.SparseAdam.step">(torch.optim.SparseAdam 方法)</a>
</li>
      </ul></li>
      <li><a href="optim.html#torch.optim.lr_scheduler.StepLR">StepLR (torch.optim.lr_scheduler 中的类)</a>
</li>
      <li><a href="torchvision/datasets.html#torchvision.datasets.STL10">STL10 (torchvision.datasets 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.storage">storage() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.storage_offset">storage_offset() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.storage_type">storage_type() (torch.Tensor 类方法)</a>
</li>
      <li><a href="cuda.html#torch.cuda.Stream">Stream (torch.cuda 中的类)</a>
</li>
      <li><a href="cuda.html#torch.cuda.stream">stream() (在 torch.cuda 模块中)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.stride">stride() (torch.Tensor 方法)</a>
</li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.sub">sub() (torch.sparse.FloatTensor 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.sub">(torch.Tensor 方法)</a>
</li>
      </ul></li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.sub_">sub_() (torch.sparse.FloatTensor 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.sub_">(torch.Tensor 方法)</a>
</li>
      </ul></li>
      <li><a href="data.html#torch.utils.data.sampler.SubsetRandomSampler">SubsetRandomSampler (torch.utils.data.sampler 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.sum">sum() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.sum">(在 torch 模块中)</a>, <a href="torch.html#torch.sum">[1]</a>, <a href="torch.html#torch.sum">[2]</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.svd">svd() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.svd">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="torchvision/datasets.html#torchvision.datasets.SVHN">SVHN (torchvision.datasets 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.symeig">symeig() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.symeig">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="cuda.html#torch.cuda.Event.synchronize">synchronize() (torch.cuda.Event 方法)</a>

      <ul>
        <li><a href="cuda.html#torch.cuda.Stream.synchronize">(torch.cuda.Stream 方法)</a>
</li>
        <li><a href="cuda.html#torch.cuda.synchronize">(在 torch.cuda 模块中)</a>
</li>
      </ul></li>
  </ul></td>
</tr></table>

<h2 id="T">T</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="tensors.html#torch.Tensor.t">t() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.t">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.t_">t_() (torch.sparse.FloatTensor 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.t_">(torch.Tensor 方法)</a>
</li>
      </ul></li>
      <li><a href="autograd.html#torch.autograd.profiler.profile.table">table() (torch.autograd.profiler.profile 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.take">take() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.take">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.tan">tan() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.tan">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.tan_">tan_() (torch.Tensor 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.Tanh">Tanh (torch.nn 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.tanh">tanh() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.tanh">(在 torch 模块中)</a>
</li>
        <li><a href="nn.html#torch.nn.functional.tanh">(在 torch.nn.functional 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.tanh_">tanh_() (torch.Tensor 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.Tanhshrink">Tanhshrink (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.tanhshrink">tanhshrink() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.TenCrop">TenCrop (torchvision.transforms 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor">Tensor (torch 中的类)</a>
</li>
      <li><a href="data.html#torch.utils.data.TensorDataset">TensorDataset (torch.utils.data 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.Threshold">Threshold (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.threshold">threshold() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.toDense">toDense() (torch.sparse.FloatTensor 方法)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.tolist">tolist() (torch.FloatStorage 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.tolist">(torch.Tensor 方法)</a>
</li>
      </ul></li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.ToPILImage">ToPILImage (torchvision.transforms 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.topk">topk() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.topk">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="torch.html#module-torch">torch (模块)</a>
</li>
      <li><a href="autograd.html#module-torch.autograd">torch.autograd (模块)</a>
</li>
      <li><a href="cuda.html#module-torch.cuda">torch.cuda (模块)</a>
</li>
      <li><a href="distributed.html#module-torch.distributed">torch.distributed (模块)</a>
</li>
      <li><a href="distributions.html#module-torch.distributions">torch.distributions (模块)</a>
</li>
      <li><a href="legacy.html#module-torch.legacy">torch.legacy (模块)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="multiprocessing.html#module-torch.multiprocessing">torch.multiprocessing (模块)</a>
</li>
      <li><a href="nn.html#module-torch.nn">torch.nn (模块)</a>
</li>
      <li><a href="onnx.html#module-torch.onnx">torch.onnx (模块)</a>
</li>
      <li><a href="optim.html#module-torch.optim">torch.optim (模块)</a>
</li>
      <li><a href="data.html#module-torch.utils.data">torch.utils.data (模块)</a>
</li>
      <li><a href="model_zoo.html#module-torch.utils.model_zoo">torch.utils.model_zoo (模块)</a>
</li>
      <li><a href="torchvision/index.html#module-torchvision">torchvision (模块)</a>
</li>
      <li><a href="autograd.html#torch.autograd.profiler.profile.total_average">total_average() (torch.autograd.profiler.profile 方法)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.ToTensor">ToTensor (torchvision.transforms 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.trace">trace() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.trace">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.Module.train">train() (torch.nn.Module 方法)</a>
</li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.transpose">transpose() (torch.sparse.FloatTensor 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.transpose">(torch.Tensor 方法)</a>
</li>
        <li><a href="torch.html#torch.transpose">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.transpose_">transpose_() (torch.sparse.FloatTensor 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.transpose_">(torch.Tensor 方法)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.tril">tril() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.tril">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.tril_">tril_() (torch.Tensor 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.triplet_margin_loss">triplet_margin_loss() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.TripletMarginLoss">TripletMarginLoss (torch.nn 中的类)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.triu">triu() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.triu">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.triu_">triu_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.trtrs">trtrs() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.trtrs">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.trunc">trunc() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.trunc">(在 torch 模块中)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.trunc_">trunc_() (torch.Tensor 方法)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.type">type() (torch.FloatStorage 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.type">(torch.Tensor 方法)</a>
</li>
        <li><a href="nn.html#torch.nn.Module.type">(torch.nn.Module 方法)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.type_as">type_as() (torch.Tensor 方法)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="U">U</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torch.html#torch.unbind">unbind() (在 torch 模块中)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.unfold">unfold() (torch.Tensor 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.init.uniform">uniform() (在 torch.nn.init 模块中)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.uniform_">uniform_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.unsqueeze">unsqueeze() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.unsqueeze">(在 torch 模块中)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="tensors.html#torch.Tensor.unsqueeze_">unsqueeze_() (torch.Tensor 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.Upsample">Upsample (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.upsample">upsample() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.upsample_bilinear">upsample_bilinear() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.upsample_nearest">upsample_nearest() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="nn.html#torch.nn.UpsamplingBilinear2d">UpsamplingBilinear2d (torch.nn 中的类)</a>
</li>
      <li><a href="nn.html#torch.nn.UpsamplingNearest2d">UpsamplingNearest2d (torch.nn 中的类)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="V">V</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="tensors.html#torch.Tensor.var">var() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="torch.html#torch.var">(在 torch 模块中)</a>, <a href="torch.html#torch.var">[1]</a>, <a href="torch.html#torch.var">[2]</a>
</li>
      </ul></li>
      <li><a href="autograd.html#torch.autograd.Variable">Variable (torch.autograd 中的类)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.vgg11">vgg11() (在 torchvision.models 模块中)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.vgg11_bn">vgg11_bn() (在 torchvision.models 模块中)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.vgg13">vgg13() (在 torchvision.models 模块中)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchvision/models.html#torchvision.models.vgg13_bn">vgg13_bn() (在 torchvision.models 模块中)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.vgg16">vgg16() (在 torchvision.models 模块中)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.vgg16_bn">vgg16_bn() (在 torchvision.models 模块中)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.vgg19">vgg19() (在 torchvision.models 模块中)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.vgg19_bn">vgg19_bn() (在 torchvision.models 模块中)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.view">view() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.view_as">view_as() (torch.Tensor 方法)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="W">W</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="cuda.html#torch.cuda.Event.wait">wait() (torch.cuda.Event 方法)</a>
</li>
      <li><a href="cuda.html#torch.cuda.Stream.wait_event">wait_event() (torch.cuda.Stream 方法)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="cuda.html#torch.cuda.Stream.wait_stream">wait_stream() (torch.cuda.Stream 方法)</a>
</li>
      <li><a href="nn.html#torch.nn.utils.weight_norm">weight_norm() (在 torch.nn.utils 模块中)</a>
</li>
      <li><a href="data.html#torch.utils.data.sampler.WeightedRandomSampler">WeightedRandomSampler (torch.utils.data.sampler 中的类)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="X">X</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.init.xavier_normal">xavier_normal() (在 torch.nn.init 模块中)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.init.xavier_uniform">xavier_uniform() (在 torch.nn.init 模块中)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="Z">Z</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="sparse.html#torch.sparse.FloatTensor.zero_">zero_() (torch.sparse.FloatTensor 方法)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.zero_">(torch.Tensor 方法)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.Module.zero_grad">zero_grad() (torch.nn.Module 方法)</a>

      <ul>
        <li><a href="optim.html#torch.optim.Optimizer.zero_grad">(torch.optim.Optimizer 方法)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.ZeroPad2d">ZeroPad2d (torch.nn 中的类)</a>
</li>
      <li><a href="torch.html#torch.zeros">zeros() (在 torch 模块中)</a>
</li>
      <li><a href="torch.html#torch.zeros_like">zeros_like() (在 torch 模块中)</a>
</li>
  </ul></td>
</tr></table>



           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Torch Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'master',
            LANGUAGE:'zh',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  <script type="text/javascript">
      jQuery(function () {
          
          SphinxRtdTheme.Navigation.enableSticky();
          
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>


</body>
</html>