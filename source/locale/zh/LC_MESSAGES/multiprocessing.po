# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2017, Torch Contributors
# This file is distributed under the same license as the PyTorch package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2018.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PyTorch master (0.3.0.post4 )\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2018-01-12 11:13+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.5.1\n"

#: ../../source/multiprocessing.rst:2
msgid "Multiprocessing package - torch.multiprocessing"
msgstr ""

#: of torch.multiprocessing:1
msgid ""
"torch.multiprocessing is a wrapper around the native "
":mod:`multiprocessing` module. It registers custom reducers, that use "
"shared memory to provide shared views on the same data in different "
"processes. Once the tensor/storage is moved to shared_memory (see "
":func:`~torch.Tensor.share_memory_`), it will be possible to send it to "
"other processes without making any copies."
msgstr ""

#: of torch.multiprocessing:7
#, python-format
msgid ""
"The API is 100% compatible with the original module - it's enough to "
"change ``import multiprocessing`` to ``import torch.multiprocessing`` to "
"have all the tensors sent through the queues or shared via other "
"mechanisms, moved to shared memory."
msgstr ""

#: of torch.multiprocessing:12
msgid ""
"Because of the similarity of APIs we do not document most of this package"
" contents, and we recommend referring to very good docs of the original "
"module."
msgstr ""

#: ../../source/multiprocessing.rst:9
msgid ""
"If the main process exits abruptly (e.g. because of an incoming signal), "
"Python's ``multiprocessing`` sometimes fails to clean up its children. "
"It's a known caveat, so if you're seeing any resource leaks after "
"interrupting the interpreter, it probably means that this has just "
"happened to you."
msgstr ""

#: ../../source/multiprocessing.rst:16
msgid "Strategy management"
msgstr ""

#: of torch.multiprocessing.get_all_sharing_strategies:1
msgid "Returns a set of sharing strategies supported on a current system."
msgstr ""

#: of torch.multiprocessing.get_sharing_strategy:1
msgid "Returns the current strategy for sharing CPU tensors."
msgstr ""

#: of torch.multiprocessing.set_sharing_strategy:1
msgid "Sets the strategy for sharing CPU tensors."
msgstr ""

#: of torch.multiprocessing.set_sharing_strategy
msgid "参数"
msgstr ""

#: of torch.multiprocessing.set_sharing_strategy:3
msgid ""
"Name of the selected strategy. Should be one of the values returned by "
":func:`get_all_sharing_strategies()`."
msgstr ""

#: ../../source/multiprocessing.rst:23
msgid "Sharing CUDA tensors"
msgstr ""

#: ../../source/multiprocessing.rst:25
msgid ""
"Sharing CUDA tensors between processes is supported only in Python 3, "
"using a ``spawn`` or ``forkserver`` start methods. "
":mod:`python:multiprocessing` in Python 2 can only create subprocesses "
"using ``fork``, and it's not supported by the CUDA runtime."
msgstr ""

#: ../../source/multiprocessing.rst:32
msgid ""
"CUDA API requires that the allocation exported to other processes remains"
" valid as long as it's used by them. You should be careful and ensure "
"that CUDA tensors you shared don't go out of scope as long as it's "
"necessary. This shouldn't be a problem for sharing model parameters, but "
"passing other kinds of data should be done with care. Note that this "
"restriction doesn't apply to shared CPU memory."
msgstr ""

#: ../../source/multiprocessing.rst:41
msgid "Sharing strategies"
msgstr ""

#: ../../source/multiprocessing.rst:43
msgid ""
"This section provides a brief overview into how different sharing "
"strategies work. Note that it applies only to CPU tensor - CUDA tensors "
"will always use the CUDA API, as that's the only way they can be shared."
msgstr ""

#: ../../source/multiprocessing.rst:48
msgid "File descriptor - ``file_descriptor``"
msgstr ""

#: ../../source/multiprocessing.rst:53
msgid ""
"This is the default strategy (except for macOS and OS X where it's not "
"supported)."
msgstr ""

#: ../../source/multiprocessing.rst:56
msgid ""
"This strategy will use file descriptors as shared memory handles. "
"Whenever a storage is moved to shared memory, a file descriptor obtained "
"from ``shm_open`` is cached with the object, and when it's going to be "
"sent to other processes, the file descriptor will be transferred (e.g. "
"via UNIX sockets) to it. The receiver will also cache the file descriptor"
" and ``mmap`` it, to obtain a shared view onto the storage data."
msgstr ""

#: ../../source/multiprocessing.rst:63
msgid ""
"Note that if there will be a lot of tensors shared, this strategy will "
"keep a large number of file descriptors open most of the time. If your "
"system has low limits for the number of open file descriptors, and you "
"can't raise them, you should use the ``file_system`` strategy."
msgstr ""

#: ../../source/multiprocessing.rst:69
msgid "File system - ``file_system``"
msgstr ""

#: ../../source/multiprocessing.rst:71
msgid ""
"This strategy will use file names given to ``shm_open`` to identify the "
"shared memory regions. This has a benefit of not requiring the "
"implementation to cache the file descriptors obtained from it, but at the"
" same time is prone to shared memory leaks. The file can't be deleted "
"right after its creation, because other processes need to access it to "
"open their views. If the processes fatally crash, or are killed, and "
"don't call the storage destructors, the files will remain in the system. "
"This is very serious, because they keep using up the memory until the "
"system is restarted, or they're freed manually."
msgstr ""

#: ../../source/multiprocessing.rst:80
msgid ""
"To counter the problem of shared memory file leaks, "
":mod:`torch.multiprocessing` will spawn a daemon named "
"``torch_shm_manager`` that will isolate itself from the current process "
"group, and will keep track of all shared memory allocations. Once all "
"processes connected to it exit, it will wait a moment to ensure there "
"will be no new connections, and will iterate over all shared memory files"
" allocated by the group. If it finds that any of them still exist, they "
"will be deallocated. We've tested this method and it proved to be robust "
"to various failures. Still, if your system has high enough limits, and "
"``file_descriptor`` is a supported strategy, we do not recommend "
"switching to this one."
msgstr ""

