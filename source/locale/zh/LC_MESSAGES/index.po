msgid ""
msgstr ""
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"X-Generator: POEditor.com\n"
"Project-Id-Version: pytorch 中文文档\n"
"Language: zh-CN\n"

#: ../../source/index.rst:9
msgid "PyTorch documentation"
msgstr "PyTorch 文档"

#: ../../source/index.rst:11
msgid "PyTorch is an optimized tensor library for deep learning using GPUs and CPUs."
msgstr "PyTorch是一个最优化的张量库，可以为深度学习提供CPU和GPU加速"

#: ../../source/index.rst:13
msgid "Notes"
msgstr "注意"

#: ../../source/index.rst:21
msgid "Package Reference"
msgstr ""

#: ../../source/index.rst:42
msgid "torchvision Reference"
msgstr ""

#: ../../source/index.rst:51
msgid "Indices and tables"
msgstr ""

#: ../../source/index.rst:53
msgid ":ref:`genindex`"
msgstr ""

#: ../../source/index.rst:54
msgid ":ref:`modindex`"
msgstr ""

#: ../../source/autograd.rst:5
msgid "Automatic differentiation package - torch.autograd"
msgstr "自动微分: torch.autograd"

#: of torch.autograd:1
msgid "torch.autograd provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions. It requires minimal changes to the existing code - you only need to wrap all tensors in :class:`.Variable` objects."
msgstr ""

#: of torch.autograd.backward:1
msgid "Computes the sum of gradients of given variables w.r.t. graph leaves."
msgstr ""

#: of torch.autograd.backward:3
msgid "The graph is differentiated using the chain rule. If any of ``variables`` are non-scalar (i.e. their data has more than one element) and require gradient, the function additionally requires specifying ``grad_variables``. It should be a sequence of matching length, that contains gradient of the differentiated function w.r.t. corresponding variables (``None`` is an acceptable value for all variables that don't need gradient tensors)."
msgstr ""

#: of torch.autograd.Variable.backward:9 torch.autograd.backward:10
msgid "This function accumulates gradients in the leaves - you might need to zero them before calling it."
msgstr ""

#: of torch.utils.data.ConcatDataset torch.utils.data.DataLoader
#: torch.utils.data.TensorDataset
#: torch.utils.data.distributed.DistributedSampler
#: torch.utils.data.sampler.RandomSampler
#: torch.utils.data.sampler.SequentialSampler
#: torch.utils.data.sampler.SubsetRandomSampler
#: torch.utils.data.sampler.WeightedRandomSampler
msgid "参数"
msgstr ""

#: of torch.autograd.backward:13
msgid "Variables of which the derivative will be computed."
msgstr ""

#: of torch.autograd.backward:16
msgid "Gradients w.r.t. each element of corresponding variables.  Any tensors will be automatically converted to Variables that are volatile unless ``create_graph`` is ``True``.  None values can be specified for scalar Variables or ones that don't require grad. If a None value would be acceptable for all grad_variables, then this argument is optional."
msgstr ""

#: of torch.autograd.backward:23 torch.autograd.grad:26
msgid "If ``False``, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to ``True`` is not needed and often can be worked around in a much more efficient way. Defaults to the value of ``create_graph``."
msgstr ""

#: of torch.autograd.backward:28 torch.autograd.grad:31
msgid "If ``True``, graph of the derivative will be constructed, allowing to compute higher order derivative products. Defaults to ``False``, unless ``grad_variables`` contains at least one non-volatile Variable."
msgstr ""

#: of torch.autograd.grad:1
msgid "Computes and returns the sum of gradients of outputs w.r.t. the inputs."
msgstr ""

#: of torch.autograd.grad:3
msgid "``grad_outputs`` should be a sequence of length matching ``output`` containing the pre-computed gradients w.r.t. each of the outputs. If an output doesn't require_grad, then the gradient can be ``None``). Gradients can be given as Tensors when one doesn't need the graph of the derivative, or as Variables, in which case the graph will be created."
msgstr ""

#: of torch.autograd.grad:9
msgid "If ``only_inputs`` is ``True``, the function will only return a list of gradients w.r.t the specified inputs. If it's ``False``, then gradient w.r.t. all remaining leaves will still be computed, and will be accumulated into their ``.grad`` attribute."
msgstr ""

#: of torch.autograd.grad:14
msgid "outputs of the differentiated function."
msgstr ""

#: of torch.autograd.grad:16
msgid "Inputs w.r.t. which the gradient will be returned (and not accumulated into ``.grad``)."
msgstr ""

#: of torch.autograd.grad:19
msgid "Gradients w.r.t. each output. Any tensors will be automatically converted to Variables that are volatile unless ``create_graph`` is ``True``. None values can be specified for scalar Variables or ones that don't require grad. If a None value would be acceptable for all grad_variables, then this argument is optional."
msgstr ""

#: of torch.autograd.grad:36
msgid "If ``True``, gradient w.r.t. leaves that are part of the graph, but don't appear in ``inputs`` won't be computed and accumulated. Defaults to ``True``."
msgstr ""

#: of torch.autograd.grad:40
msgid "If ``False``, specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is an error. Defaults to ``False``."
msgstr ""

#: ../../source/autograd.rst:15
msgid "Variable"
msgstr ""

#: ../../source/autograd.rst:18
msgid "API compatibility"
msgstr ""

#: ../../source/autograd.rst:20
msgid "Variable API is nearly the same as regular Tensor API (with the exception of a couple in-place methods, that would overwrite inputs required for gradient computation). In most cases Tensors can be safely replaced with Variables and the code will remain to work just fine. Because of this, we're not documenting all the operations on variables, and you should refer to :class:`torch.Tensor` docs for this purpose."
msgstr ""

#: ../../source/autograd.rst:28
msgid "In-place operations on Variables"
msgstr ""

#: ../../source/autograd.rst:30
msgid "Supporting in-place operations in autograd is a hard matter, and we discourage their use in most cases. Autograd's aggressive buffer freeing and reuse makes it very efficient and there are very few occasions when in-place operations actually lower memory usage by any significant amount. Unless you're operating under heavy memory pressure, you might never need to use them."
msgstr ""

#: ../../source/autograd.rst:37
msgid "In-place correctness checks"
msgstr ""

#: ../../source/autograd.rst:39
msgid "All :class:`Variable` s keep track of in-place operations applied to them, and if the implementation detects that a variable was saved for backward in one of the functions, but it was modified in-place afterwards, an error will be raised once backward pass is started. This ensures that if you're using in-place functions and not seeing any errors, you can be sure that the computed gradients are correct."
msgstr ""

#: of torch.autograd.Variable:1
msgid "Wraps a tensor and records the operations applied to it."
msgstr ""

#: of torch.autograd.Variable:3
msgid "Variable is a thin wrapper around a Tensor object, that also holds the gradient w.r.t. to it, and a reference to a function that created it. This reference allows retracing the whole chain of operations that created the data. If the Variable has been created by the user, its grad_fn will be ``None`` and we call such objects *leaf* Variables."
msgstr ""

#: of torch.autograd.Variable:9
msgid "Since autograd only supports scalar valued function differentiation, grad size always matches the data size. Also, grad is normally only allocated for leaf variables, and will be always zero otherwise."
msgstr ""

#: of torch.autograd.Function torch.autograd.Variable
msgid "变量"
msgstr ""

#: of torch.autograd.Variable:13
msgid "Wrapped tensor of any type."
msgstr ""

#: of torch.autograd.Variable:14
msgid "Variable holding the gradient of type and location matching the ``.data``.  This attribute is lazily allocated and can't be reassigned."
msgstr ""

#: of torch.autograd.Variable:17
msgid "Boolean indicating whether the Variable has been created by a subgraph containing any Variable, that requires it. See :ref:`excluding-subgraphs` for more details. Can be changed only on leaf Variables."
msgstr ""

#: of torch.autograd.Variable:21
msgid "Boolean indicating that the Variable should be used in inference mode, i.e. don't save the history. See :ref:`excluding-subgraphs` for more details. Can be changed only on leaf Variables."
msgstr ""

#: of torch.autograd.Variable:25
msgid "Boolean indicating if the Variable is a graph leaf (i.e if it was created by the user)."
msgstr ""

#: of torch.autograd.Variable:27
msgid "Gradient function graph trace."
msgstr ""

#: of torch.autograd.Variable:30
msgid "Tensor to wrap."
msgstr ""

#: of torch.autograd.Variable:32
msgid "Value of the requires_grad flag. **Keyword only.**"
msgstr ""

#: of torch.autograd.Variable:34
msgid "Value of the volatile flag. **Keyword only.**"
msgstr ""

#: of torch.autograd.Variable.backward:1
msgid "Computes the gradient of current variable w.r.t. graph leaves."
msgstr ""

#: of torch.autograd.Variable.backward:3
msgid "The graph is differentiated using the chain rule. If the variable is non-scalar (i.e. its data has more than one element) and requires gradient, the function additionally requires specifying ``gradient``. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. ``self``."
msgstr ""

#: of torch.autograd.Variable.backward:12
msgid "Gradient w.r.t. the variable. If it is a tensor, it will be automatically converted to a Variable that is volatile unless ``create_graph`` is True. None values can be specified for scalar Variables or ones that don't require grad. If a None value would be acceptable then this argument is optional."
msgstr ""

#: of torch.autograd.Variable.backward:19
msgid "If ``False``, the graph used to compute the grads will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of ``create_graph``."
msgstr ""

#: of torch.autograd.Variable.backward:25
msgid "If ``True``, graph of the derivative will be constructed, allowing to compute higher order derivative products. Defaults to ``False``, unless ``gradient`` is a volatile Variable."
msgstr ""

#: of torch.autograd.Variable.detach:1
msgid "Returns a new Variable, detached from the current graph."
msgstr ""

#: of torch.autograd.Variable.detach:3
msgid "Result will never require gradient. If the input is volatile, the output will be volatile too."
msgstr ""

#: of torch.autograd.Variable.detach:8
msgid "Returned Variable uses the same data tensor, as the original one, and in-place modifications on either of them will be seen, and may trigger errors in correctness checks."
msgstr ""

#: of torch.autograd.Variable.detach_:1
msgid "Detaches the Variable from the graph that created it, making it a leaf."
msgstr ""

#: of torch.autograd.Variable.register_hook:1
msgid "Registers a backward hook."
msgstr ""

#: of torch.autograd.Variable.register_hook:3
msgid "The hook will be called every time a gradient with respect to the variable is computed. The hook should have the following signature::"
msgstr ""

#: of torch.autograd.Variable.register_hook:8
msgid "The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of :attr:`grad`."
msgstr ""

#: of torch.autograd.Variable.register_hook:11
msgid "This function returns a handle with a method ``handle.remove()`` that removes the hook from the module."
msgstr ""

#: of torch.autograd.Variable.register_hook:15
#: torch.autograd.profiler.emit_nvtx:23 torch.autograd.profiler.profile:12
msgid "Example"
msgstr ""

#: of torch.autograd.Variable.retain_grad:1
msgid "Enables .grad attribute for non-leaf Variables."
msgstr ""

#: ../../source/autograd.rst:51
msgid ":hidden:`Function`"
msgstr ""

#: of torch.autograd.Function:1
msgid "Records operation history and defines formulas for differentiating ops."
msgstr ""

#: of torch.autograd.Function:3
msgid "Every operation performed on :class:`Variable` s creates a new function object, that performs the computation, and records that it happened. The history is retained in the form of a DAG of functions, with edges denoting data dependencies (``input <- output``). Then, when backward is called, the graph is processed in the topological ordering, by calling :func:`backward` methods of each :class:`Function` object, and passing returned gradients on to next :class:`Function` s."
msgstr ""

#: of torch.autograd.Function:11
msgid "Normally, the only way users interact with functions is by creating subclasses and defining new operations. This is a recommended way of extending torch.autograd."
msgstr ""

#: of torch.autograd.Function:15
msgid "Each function is meant to be used only once (in the forward pass)."
msgstr ""

#: of torch.autograd.Function:17
msgid "Boolean indicating whether the :func:`backward` will ever need to be called."
msgstr ""

#: of torch.autograd.Function:21
msgid "Examples::"
msgstr ""

#: of torch.autograd.Function.backward:1
msgid "Defines a formula for differentiating the operation."
msgstr ""

#: of torch.autograd.Function.backward:3 torch.autograd.Function.forward:3
msgid "This function is to be overriden by all subclasses."
msgstr ""

#: of torch.autograd.Function.backward:5
msgid "It must accept a context ctx as the first argument, followed by as many outputs did :func:`forward` return, and it should return as many tensors, as there were inputs to :func:`forward`. Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input."
msgstr ""

#: of torch.autograd.Function.backward:11
msgid "The context can be used to retrieve variables saved during the forward pass."
msgstr ""

#: of torch.autograd.Function.forward:1
msgid "Performs the operation."
msgstr ""

#: of torch.autograd.Function.forward:5
msgid "It must accept a context ctx as the first argument, followed by any number of arguments (tensors or other types)."
msgstr ""

#: of torch.autograd.Function.forward:8
msgid "The context can be used to store variables that can be then retrieved during the backward pass."
msgstr ""

#: ../../source/autograd.rst:57
msgid "Profiler"
msgstr ""

#: ../../source/autograd.rst:59
msgid "Autograd includes a profiler that lets you inspect the cost of different operators inside your model - both on the CPU and GPU. There are two modes implemented at the moment - CPU-only using :class:`~torch.autograd.profiler.profile`. and nvprof based (registers both CPU and GPU activity) using :class:`~torch.autograd.profiler.emit_nvtx`."
msgstr ""

#: of torch.autograd.profiler.profile:1
msgid "Context manager that manages autograd profiler state and holds a summary of results."
msgstr ""

#: of torch.autograd.profiler.emit_nvtx:18 torch.autograd.profiler.profile:3
msgid "Setting this to False makes this context manager a no-op. Default: ``True``."
msgstr ""

#: of torch.autograd.profiler.profile.export_chrome_trace:1
msgid "Exports an EventList as a Chrome tracing tools file."
msgstr ""

#: of torch.autograd.profiler.profile.export_chrome_trace:3
msgid "The checkpoint can be later loaded and inspected under ``chrome://tracing`` URL."
msgstr ""

#: of torch.autograd.profiler.profile.export_chrome_trace:5
msgid "Path where the trace will be written."
msgstr ""

#: of torch.autograd.profiler.profile.key_averages:1
msgid "Averages all function events over their keys."
msgstr ""

#: of torch.cuda.Event.query torch.cuda.Stream.query
#: torch.cuda.Stream.record_event torch.cuda.comm.broadcast
#: torch.cuda.comm.gather torch.cuda.comm.reduce_add torch.cuda.comm.scatter
#: torch.cuda.get_device_capability
msgid "返回"
msgstr ""

#: of torch.autograd.profiler.profile.key_averages:3
msgid "An EventList containing FunctionEventAvg objects."
msgstr ""

#: of torch.autograd.profiler.profile.table:1
msgid "Prints an EventList as a nicely formatted table."
msgstr ""

#: of torch.autograd.profiler.profile.table:3
msgid "Attribute used to sort entries. By default they are printed in the same order as they were registered. Valid keys include: ``cpu_time``, ``cuda_time``, ``cpu_time_total``, ``cuda_time_total``, ``count``."
msgstr ""

#: of torch.autograd.profiler.profile.table:9
msgid "A string containing the table."
msgstr ""

#: of torch.autograd.profiler.profile.total_average:1
msgid "Averages all events."
msgstr ""

#: of torch.autograd.profiler.profile.total_average:3
msgid "A FunctionEventAvg object."
msgstr ""

#: of torch.autograd.profiler.emit_nvtx:1
msgid "Context manager that makes every autograd operation emit an NVTX range."
msgstr ""

#: of torch.autograd.profiler.emit_nvtx:3
msgid "It is useful when running the program under nvprof::"
msgstr ""

#: of torch.autograd.profiler.emit_nvtx:7
msgid "Unfortunately, there's no way to force nvprof to flush the data it collected to disk, so for CUDA profiling one has to use this context manager to annotate nvprof traces and wait for the process to exit before inspecting them. Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or :func:`torch.autograd.profiler.load_nvprof` can load the results for inspection e.g. in Python REPL."
msgstr ""

#: of torch.autograd.profiler.load_nvprof:1
msgid "Opens an nvprof trace file and parses autograd annotations."
msgstr ""

#: of torch.autograd.profiler.load_nvprof:3
msgid "path to nvprof trace"
msgstr ""

#: ../../source/cuda.rst:2
msgid "torch.cuda"
msgstr ""

#: of torch.cuda:1
msgid "This package adds support for CUDA tensor types, that implement the same function as CPU tensors, but they utilize GPUs for computation."
msgstr ""

#: of torch.cuda:4
msgid "It is lazily initialized, so you can always import it, and use :func:`is_available()` to determine if your system supports CUDA."
msgstr ""

#: of torch.cuda:7
msgid ":ref:`cuda-semantics` has more details about working with CUDA."
msgstr ""

#: of torch.cuda.current_blas_handle:1
msgid "Returns cublasHandle_t pointer to current cuBLAS handle"
msgstr ""

#: of torch.cuda.current_device:1
msgid "Returns the index of a currently selected device."
msgstr ""

#: of torch.cuda.current_stream:1
msgid "Returns a currently selected :class:`Stream`."
msgstr ""

#: of torch.cuda.device:1
msgid "Context-manager that changes the selected device."
msgstr ""

#: of torch.cuda.device:3
msgid "device index to select. It's a no-op if this argument is negative."
msgstr ""

#: of torch.cuda.device_count:1
msgid "Returns the number of GPUs available."
msgstr ""

#: of torch.cuda.device_of:1
msgid "Context-manager that changes the current device to that of given object."
msgstr ""

#: of torch.cuda.device_of:3
msgid "You can use both tensors and storages as arguments. If a given object is not allocated on a GPU, this is a no-op."
msgstr ""

#: of torch.cuda.device_of:6
msgid "object allocated on the selected device."
msgstr ""

#: of torch.cuda.empty_cache:1
msgid "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in `nvidia-smi`."
msgstr ""

#: of torch.cuda.get_device_capability:1
msgid "Gets the cuda capability of a device."
msgstr ""

#: of torch.cuda.get_device_capability:3 torch.cuda.get_device_name:3
msgid "device for which to return the name. This function is a no-op if this argument is negative."
msgstr ""

#: of torch.cuda.get_device_capability:7
msgid "the major and minor cuda capability of the device"
msgstr ""

#: of torch.cuda.get_device_capability
msgid "返回类型"
msgstr ""

#: of torch.cuda.get_device_name:1
msgid "Gets the name of a device."
msgstr ""

#: of torch.cuda.is_available:1
msgid "Returns a bool indicating if CUDA is currently available."
msgstr ""

#: of torch.cuda.set_device:1
msgid "Sets the current device."
msgstr ""

#: of torch.cuda.set_device:3
msgid "Usage of this function is discouraged in favor of :any:`device`. In most cases it's better to use ``CUDA_VISIBLE_DEVICES`` environmental variable."
msgstr ""

#: of torch.cuda.set_device:6
msgid "selected device. This function is a no-op if this argument is negative."
msgstr ""

#: of torch.cuda.stream:1
msgid "Context-manager that selects a given stream."
msgstr ""

#: of torch.cuda.stream:3
msgid "All CUDA kernels queued within its context will be enqueued on a selected stream."
msgstr ""

#: of torch.cuda.stream:6
msgid "selected stream. This manager is a no-op if it's ``None``."
msgstr ""

#: of torch.cuda.synchronize:1
msgid "Waits for all kernels in all streams on current device to complete."
msgstr ""

#: ../../source/cuda.rst:10
msgid "Random Number Generator"
msgstr ""

#: of torch.cuda.get_rng_state:1
msgid "Returns the random number generator state of the current GPU as a ByteTensor."
msgstr ""

#: of torch.cuda.get_rng_state:4
msgid "The device to return the RNG state of. Default: -1 (i.e., use the current device)."
msgstr ""

#: of torch.cuda.get_rng_state:9 torch.cuda.initial_seed:4
msgid "This function eagerly initializes CUDA."
msgstr ""

#: of torch.cuda.set_rng_state:1
msgid "Sets the random number generator state of the current GPU."
msgstr ""

#: of torch.cuda.set_rng_state:3
msgid "The desired state"
msgstr ""

#: of torch.cuda.manual_seed:1
msgid "Sets the seed for generating random numbers for the current GPU. It's safe to call this function if CUDA is not available; in that case, it is silently ignored."
msgstr ""

#: of torch.cuda.manual_seed:5 torch.cuda.manual_seed_all:5
msgid "The desired seed."
msgstr ""

#: of torch.cuda.manual_seed:9
msgid "If you are working with a multi-GPU model, this function is insufficient to get determinism.  To seed all GPUs, use :func:`manual_seed_all`."
msgstr ""

#: of torch.cuda.manual_seed_all:1
msgid "Sets the seed for generating random numbers on all GPUs. It's safe to call this function if CUDA is not available; in that case, it is silently ignored."
msgstr ""

#: of torch.cuda.seed:1
msgid "Sets the seed for generating random numbers to a random number for the current GPU. It's safe to call this function if CUDA is not available; in that case, it is silently ignored."
msgstr ""

#: of torch.cuda.seed:6
msgid "If you are working with a multi-GPU model, this function will only initialize the seed on one GPU.  To initialize all GPUs, use :func:`seed_all`."
msgstr ""

#: of torch.cuda.seed_all:1
msgid "Sets the seed for generating random numbers to a random number on all GPUs. It's safe to call this function if CUDA is not available; in that case, it is silently ignored."
msgstr ""

#: of torch.cuda.initial_seed:1
msgid "Returns the current random seed of the current GPU."
msgstr ""

#: ../../source/cuda.rst:21
msgid "Communication collectives"
msgstr ""

#: of torch.cuda.comm.broadcast:1
msgid "Broadcasts a tensor to a number of GPUs."
msgstr ""

#: of torch.cuda.comm.broadcast:3
msgid "tensor to broadcast."
msgstr ""

#: of torch.cuda.comm.broadcast:5
msgid "an iterable of devices among which to broadcast. Note that it should be like (src, dst1, dst2, ...), the first element of which is the source device to broadcast from."
msgstr ""

#: of torch.cuda.comm.broadcast:10
msgid "A tuple containing copies of the ``tensor``, placed on devices corresponding to indices from ``devices``."
msgstr ""

#: of torch.cuda.comm.reduce_add:1
msgid "Sums tensors from multiple GPUs."
msgstr ""

#: of torch.cuda.comm.reduce_add:3
msgid "All inputs should have matching shapes."
msgstr ""

#: of torch.cuda.comm.reduce_add:5
msgid "an iterable of tensors to add."
msgstr ""

#: of torch.cuda.comm.reduce_add:7
msgid "a device on which the output will be placed (default: current device)."
msgstr ""

#: of torch.cuda.comm.reduce_add:11
msgid "A tensor containing an elementwise sum of all inputs, placed on the ``destination`` device."
msgstr ""

#: of torch.cuda.comm.scatter:1
msgid "Scatters tensor across multiple GPUs."
msgstr ""

#: of torch.cuda.comm.scatter:3
msgid "tensor to scatter."
msgstr ""

#: of torch.cuda.comm.scatter:5
msgid "iterable of ints, specifying among which devices the tensor should be scattered."
msgstr ""

#: of torch.cuda.comm.scatter:8
msgid "sizes of chunks to be placed on each device. It should match ``devices`` in length and sum to ``tensor.size(dim)``. If not specified, the tensor will be divided into equal chunks."
msgstr ""

#: of torch.cuda.comm.scatter:13
msgid "A dimension along which to chunk the tensor."
msgstr ""

#: of torch.cuda.comm.scatter:16
msgid "A tuple containing chunks of the ``tensor``, spread across given ``devices``."
msgstr ""

#: of torch.cuda.comm.gather:1
msgid "Gathers tensors from multiple GPUs."
msgstr ""

#: of torch.cuda.comm.gather:3
msgid "Tensor sizes in all dimension different than ``dim`` have to match."
msgstr ""

#: of torch.cuda.comm.gather:5
msgid "iterable of tensors to gather."
msgstr ""

#: of torch.cuda.comm.gather:7
msgid "a dimension along which the tensors will be concatenated."
msgstr ""

#: of torch.cuda.comm.gather:9
msgid "output device (-1 means CPU, default: current device)"
msgstr ""

#: of torch.cuda.comm.gather:13
msgid "A tensor located on ``destination`` device, that is a result of concatenating ``tensors`` along ``dim``."
msgstr ""

#: ../../source/cuda.rst:32
msgid "Streams and events"
msgstr ""

#: of torch.cuda.Stream:1
msgid "Wrapper around a CUDA stream."
msgstr ""

#: of torch.cuda.Stream:3
msgid "a device on which to allocate the Stream."
msgstr ""

#: of torch.cuda.Stream:5
msgid "priority of the stream. Lower numbers represent higher priorities."
msgstr ""

#: of torch.cuda.Stream.query:1
msgid "Checks if all the work submitted has been completed."
msgstr ""

#: of torch.cuda.Stream.query:3
msgid "A boolean indicating if all kernels in this stream are completed."
msgstr ""

#: of torch.cuda.Stream.record_event:1
msgid "Records an event."
msgstr ""

#: of torch.cuda.Stream.record_event:3
msgid "event to record. If not given, a new one will be allocated."
msgstr ""

#: of torch.cuda.Stream.record_event:7
msgid "Recorded event."
msgstr ""

#: of torch.cuda.Stream.synchronize:1
msgid "Wait for all the kernels in this stream to complete."
msgstr ""

#: of torch.cuda.Stream.wait_event:1
msgid "Makes all future work submitted to the stream wait for an event."
msgstr ""

#: of torch.cuda.Stream.wait_event:3
msgid "an event to wait for."
msgstr ""

#: of torch.cuda.Stream.wait_stream:1
msgid "Synchronizes with another stream."
msgstr ""

#: of torch.cuda.Stream.wait_stream:3
msgid "All future work submitted to this stream will wait until all kernels submitted to a given stream at the time of call complete."
msgstr ""

#: of torch.cuda.Stream.wait_stream:6
msgid "a stream to synchronize."
msgstr ""

#: of torch.cuda.Event:1
msgid "Wrapper around CUDA event."
msgstr ""

#: of torch.cuda.Event:3
msgid "indicates if the event should measure time (default: ``False``)"
msgstr ""

#: of torch.cuda.Event:6
msgid "if ``True``, :meth:`wait` will be blocking (default: ``False``)"
msgstr ""

#: of torch.cuda.Event:8
msgid "if ``True``, the event can be shared between processes (default: ``False``)"
msgstr ""

#: of torch.cuda.Event.elapsed_time:1
msgid "Returns the time elapsed before the event was recorded."
msgstr ""

#: of torch.cuda.Event.ipc_handle:1
msgid "Returns an IPC handle of this event."
msgstr ""

#: of torch.cuda.Event.query:1
msgid "Checks if the event has been recorded."
msgstr ""

#: of torch.cuda.Event.query:3
msgid "A boolean indicating if the event has been recorded."
msgstr ""

#: of torch.cuda.Event.record:1
msgid "Records the event in a given stream."
msgstr ""

#: of torch.cuda.Event.synchronize:1
msgid "Synchronizes with the event."
msgstr ""

#: of torch.cuda.Event.wait:1
msgid "Makes a given stream wait for the event."
msgstr ""

#: ../../source/cuda.rst:41
msgid "Memory management"
msgstr ""

#: ../../source/cuda.rst:45
msgid "NVIDIA Tools Extension (NVTX)"
msgstr ""

#: of torch.cuda.nvtx.mark:1
msgid "Describe an instantaneous event that occurred at some point."
msgstr ""

#: of torch.cuda.nvtx.mark:3
msgid "ASCII message to associate with the event."
msgstr ""

#: of torch.cuda.nvtx.range_push:1
msgid "Pushes a range onto a stack of nested range span.  Returns zero-based depth of the range that is started."
msgstr ""

#: of torch.cuda.nvtx.range_push:4
msgid "ASCII message to associate with range"
msgstr ""

#: of torch.cuda.nvtx.range_pop:1
msgid "Pops a range off of a stack of nested range spans.  Returns the zero-based depth of the range that is ended."
msgstr ""

#: ../../source/data.rst:2
msgid "torch.utils.data"
msgstr ""

#: of torch.utils.data.Dataset:1
msgid "An abstract class representing a Dataset."
msgstr ""

#: of torch.utils.data.Dataset:3
msgid "All other datasets should subclass it. All subclasses should override ``__len__``, that provides the size of the dataset, and ``__getitem__``, supporting integer indexing in range from 0 to len(self) exclusive."
msgstr ""

#: of torch.utils.data.TensorDataset:1
msgid "Dataset wrapping data and target tensors."
msgstr ""

#: of torch.utils.data.TensorDataset:3
msgid "Each sample will be retrieved by indexing both tensors along the first dimension."
msgstr ""

#: of torch.utils.data.TensorDataset:6
msgid "contains sample data."
msgstr ""

#: of torch.utils.data.TensorDataset:8
msgid "contains sample targets (labels)."
msgstr ""

#: of torch.utils.data.ConcatDataset:1
msgid "Dataset to concatenate multiple datasets. Purpose: useful to assemble different existing datasets, possibly large-scale datasets as the concatenation operation is done in an on-the-fly manner."
msgstr ""

#: of torch.utils.data.ConcatDataset:6
msgid "List of datasets to be concatenated"
msgstr ""

#: of torch.utils.data.DataLoader:1
msgid "Data loader. Combines a dataset and a sampler, and provides single- or multi-process iterators over the dataset."
msgstr ""

#: of torch.utils.data.DataLoader:4
msgid "dataset from which to load the data."
msgstr ""

#: of torch.utils.data.DataLoader:6
msgid "how many samples per batch to load (default: 1)."
msgstr ""

#: of torch.utils.data.DataLoader:9
msgid "set to ``True`` to have the data reshuffled at every epoch (default: False)."
msgstr ""

#: of torch.utils.data.DataLoader:12
msgid "defines the strategy to draw samples from the dataset. If specified, ``shuffle`` must be False."
msgstr ""

#: of torch.utils.data.DataLoader:15
msgid "like sampler, but returns a batch of indices at a time. Mutually exclusive with batch_size, shuffle, sampler, and drop_last."
msgstr ""

#: of torch.utils.data.DataLoader:19
msgid "how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process (default: 0)"
msgstr ""

#: of torch.utils.data.DataLoader:23
msgid "merges a list of samples to form a mini-batch."
msgstr ""

#: of torch.utils.data.DataLoader:25
msgid "If ``True``, the data loader will copy tensors into CUDA pinned memory before returning them."
msgstr ""

#: of torch.utils.data.DataLoader:28
msgid "set to ``True`` to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If ``False`` and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: False)"
msgstr ""

#: of torch.utils.data.sampler.Sampler:1
msgid "Base class for all Samplers."
msgstr ""

#: of torch.utils.data.sampler.Sampler:3
msgid "Every Sampler subclass has to provide an __iter__ method, providing a way to iterate over indices of dataset elements, and a __len__ method that returns the length of the returned iterators."
msgstr ""

#: of torch.utils.data.sampler.SequentialSampler:1
msgid "Samples elements sequentially, always in the same order."
msgstr ""

#: of torch.utils.data.sampler.RandomSampler:3
#: torch.utils.data.sampler.SequentialSampler:3
msgid "dataset to sample from"
msgstr ""

#: of torch.utils.data.sampler.RandomSampler:1
msgid "Samples elements randomly, without replacement."
msgstr ""

#: of torch.utils.data.sampler.SubsetRandomSampler:1
msgid "Samples elements randomly from a given list of indices, without replacement."
msgstr ""

#: of torch.utils.data.sampler.SubsetRandomSampler:3
msgid "a list of indices"
msgstr ""

#: of torch.utils.data.sampler.WeightedRandomSampler:1
msgid "Samples elements from [0,..,len(weights)-1] with given probabilities (weights)."
msgstr ""

#: of torch.utils.data.sampler.WeightedRandomSampler:3
msgid "a list of weights, not necessary summing up to one"
msgstr ""

#: of torch.utils.data.sampler.WeightedRandomSampler:5
msgid "number of samples to draw"
msgstr ""

#: of torch.utils.data.sampler.WeightedRandomSampler:7
msgid "if ``True``, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a sample index is drawn for a row, it cannot be drawn again for that row."
msgstr ""

#: of torch.utils.data.distributed.DistributedSampler:1
msgid "Sampler that restricts data loading to a subset of the dataset."
msgstr ""

#: of torch.utils.data.distributed.DistributedSampler:3
msgid "It is especially useful in conjunction with :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each process can pass a DistributedSampler instance as a DataLoader sampler, and load a subset of the original dataset that is exclusive to it."
msgstr ""

#: of torch.utils.data.distributed.DistributedSampler:9
msgid "Dataset is assumed to be of constant size."
msgstr ""

#: of torch.utils.data.distributed.DistributedSampler:11
msgid "Dataset used for sampling."
msgstr ""

#: of torch.utils.data.distributed.DistributedSampler:12
msgid "Number of processes participating in distributed training."
msgstr ""

#: of torch.utils.data.distributed.DistributedSampler:15
msgid "Rank of the current process within num_replicas."
msgstr ""

