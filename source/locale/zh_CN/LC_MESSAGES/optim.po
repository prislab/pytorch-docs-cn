# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2017, Torch Contributors
# This file is distributed under the same license as the PyTorch package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2018.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PyTorch master (0.3.0.post4 )\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2018-01-12 11:13+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.5.1\n"

#: ../../source/optim.rst:2
msgid "torch.optim"
msgstr ""

#: of torch.optim:1
msgid ""
":mod:`torch.optim` is a package implementing various optimization "
"algorithms. Most commonly used methods are already supported, and the "
"interface is general enough, so that more sophisticated ones can be also "
"easily integrated in the future."
msgstr ""

#: ../../source/optim.rst:7
msgid "How to use an optimizer"
msgstr ""

#: ../../source/optim.rst:9
msgid ""
"To use :mod:`torch.optim` you have to construct an optimizer object, that"
" will hold the current state and will update the parameters based on the "
"computed gradients."
msgstr ""

#: ../../source/optim.rst:13
msgid "Constructing it"
msgstr ""

#: ../../source/optim.rst:15
msgid ""
"To construct an :class:`Optimizer` you have to give it an iterable "
"containing the parameters (all should be "
":class:`~torch.autograd.Variable` s) to optimize. Then, you can specify "
"optimizer-specific options such as the learning rate, weight decay, etc."
msgstr ""

#: ../../source/optim.rst:21
msgid ""
"If you need to move a model to GPU via `.cuda()`, please do so before "
"constructing optimizers for it. Parameters of a model after `.cuda()` "
"will be different objects with those before the call."
msgstr ""

#: ../../source/optim.rst:25
msgid ""
"In general, you should make sure that optimized parameters live in "
"consistent locations when optimizers are constructed and used."
msgstr ""

#: ../../source/optim.rst:28 ../../source/optim.rst:75
#: ../../source/optim.rst:92
msgid "Example::"
msgstr ""

#: ../../source/optim.rst:34
msgid "Per-parameter options"
msgstr ""

#: ../../source/optim.rst:36
msgid ""
":class:`Optimizer` s also support specifying per-parameter options. To do"
" this, instead of passing an iterable of "
":class:`~torch.autograd.Variable` s, pass in an iterable of :class:`dict`"
" s. Each of them will define a separate parameter group, and should "
"contain a ``params`` key, containing a list of parameters belonging to "
"it. Other keys should match the keyword arguments accepted by the "
"optimizers, and will be used as optimization options for this group."
msgstr ""

#: ../../source/optim.rst:45
msgid ""
"You can still pass options as keyword arguments. They will be used as "
"defaults, in the groups that didn't override them. This is useful when "
"you only want to vary a single option, while keeping all others "
"consistent between parameter groups."
msgstr ""

#: ../../source/optim.rst:51
msgid ""
"For example, this is very useful when one wants to specify per-layer "
"learning rates::"
msgstr ""

#: ../../source/optim.rst:58
msgid ""
"This means that ``model.base``'s parameters will use the default learning"
" rate of ``1e-2``, ``model.classifier``'s parameters will use a learning "
"rate of ``1e-3``, and a momentum of ``0.9`` will be used for all "
"parameters"
msgstr ""

#: ../../source/optim.rst:63
msgid "Taking an optimization step"
msgstr ""

#: ../../source/optim.rst:65
msgid ""
"All optimizers implement a :func:`~Optimizer.step` method, that updates "
"the parameters. It can be used in two ways:"
msgstr ""

#: ../../source/optim.rst:69
msgid "``optimizer.step()``"
msgstr ""

#: ../../source/optim.rst:71
msgid ""
"This is a simplified version supported by most optimizers. The function "
"can be called once the gradients are computed using e.g. "
":func:`~torch.autograd.Variable.backward`."
msgstr ""

#: ../../source/optim.rst:85
msgid "``optimizer.step(closure)``"
msgstr ""

#: ../../source/optim.rst:87
msgid ""
"Some optimization algorithms such as Conjugate Gradient and LBFGS need to"
" reevaluate the function multiple times, so you have to pass in a closure"
" that allows them to recompute your model. The closure should clear the "
"gradients, compute the loss, and return it."
msgstr ""

#: ../../source/optim.rst:104
msgid "Algorithms"
msgstr ""

#: of torch.optim.Optimizer:1
msgid "Base class for all optimizers."
msgstr ""

#: of torch.optim.ASGD torch.optim.ASGD.step torch.optim.Adadelta
#: torch.optim.Adadelta.step torch.optim.Adagrad torch.optim.Adagrad.step
#: torch.optim.Adam torch.optim.Adam.step torch.optim.Adamax
#: torch.optim.Adamax.step torch.optim.LBFGS torch.optim.LBFGS.step
#: torch.optim.Optimizer torch.optim.Optimizer.add_param_group
#: torch.optim.Optimizer.load_state_dict torch.optim.Optimizer.step
#: torch.optim.RMSprop torch.optim.RMSprop.step torch.optim.Rprop
#: torch.optim.Rprop.step torch.optim.SGD torch.optim.SGD.step
#: torch.optim.SparseAdam torch.optim.SparseAdam.step
#: torch.optim.lr_scheduler.ExponentialLR torch.optim.lr_scheduler.LambdaLR
#: torch.optim.lr_scheduler.MultiStepLR
#: torch.optim.lr_scheduler.ReduceLROnPlateau torch.optim.lr_scheduler.StepLR
msgid "参数"
msgstr ""

#: of torch.optim.Optimizer:3
msgid ""
"an iterable of :class:`Variable` s or :class:`dict` s. Specifies what "
"Variables should be optimized."
msgstr ""

#: of torch.optim.Optimizer:6
msgid ""
"(dict): a dict containing default values of optimization options (used "
"when a parameter group doesn't specify them)."
msgstr ""

#: of torch.optim.Optimizer.add_param_group:1
msgid "Add a param group to the :class:`Optimizer` s `param_groups`."
msgstr ""

#: of torch.optim.Optimizer.add_param_group:3
msgid ""
"This can be useful when fine tuning a pre-trained network as frozen "
"layers can be made trainable and added to the :class:`Optimizer` as "
"training progresses."
msgstr ""

#: of torch.optim.Optimizer.add_param_group:6
msgid "Specifies what Variables should be optimized along with group"
msgstr ""

#: of torch.optim.Optimizer.load_state_dict:1
msgid "Loads the optimizer state."
msgstr ""

#: of torch.optim.Optimizer.load_state_dict:3
msgid ""
"optimizer state. Should be an object returned from a call to "
":meth:`state_dict`."
msgstr ""

#: of torch.optim.Optimizer.state_dict:1
msgid "Returns the state of the optimizer as a :class:`dict`."
msgstr ""

#: of torch.optim.Optimizer.state_dict:3
msgid "It contains two entries:"
msgstr ""

#: of torch.optim.Optimizer.state_dict:5
msgid "state - a dict holding current optimization state. Its content"
msgstr ""

#: of torch.optim.Optimizer.state_dict:6
msgid "differs between optimizer classes."
msgstr ""

#: of torch.optim.Optimizer.state_dict:7
msgid "param_groups - a dict containing all parameter groups"
msgstr ""

#: of torch.optim.Optimizer.step:1
msgid "Performs a single optimization step (parameter update)."
msgstr ""

#: of torch.optim.Optimizer.step:3
msgid ""
"A closure that reevaluates the model and returns the loss. Optional for "
"most optimizers."
msgstr ""

#: of torch.optim.Optimizer.zero_grad:1
msgid "Clears the gradients of all optimized :class:`Variable` s."
msgstr ""

#: of torch.optim.Adadelta:1
msgid "Implements Adadelta algorithm."
msgstr ""

#: of torch.optim.Adadelta:3
msgid "It has been proposed in `ADADELTA: An Adaptive Learning Rate Method`__."
msgstr ""

#: of torch.optim.ASGD:6 torch.optim.Adadelta:5 torch.optim.Adagrad:6
#: torch.optim.Adam:5 torch.optim.Adamax:5 torch.optim.RMSprop:9
#: torch.optim.Rprop:3 torch.optim.SGD:6 torch.optim.SparseAdam:6
msgid "iterable of parameters to optimize or dicts defining parameter groups"
msgstr ""

#: of torch.optim.Adadelta:8
msgid ""
"coefficient used for computing a running average of squared gradients "
"(default: 0.9)"
msgstr ""

#: of torch.optim.Adadelta:11
msgid ""
"term added to the denominator to improve numerical stability (default: "
"1e-6)"
msgstr ""

#: of torch.optim.Adadelta:14
msgid ""
"coefficient that scale delta before it is applied to the parameters "
"(default: 1.0)"
msgstr ""

#: of torch.optim.ASGD:17 torch.optim.Adadelta:17 torch.optim.Adagrad:13
#: torch.optim.Adam:16 torch.optim.Adamax:16 torch.optim.RMSprop:24
#: torch.optim.SGD:13
msgid "weight decay (L2 penalty) (default: 0)"
msgstr ""

#: of torch.optim.ASGD.step:1 torch.optim.Adadelta.step:1
#: torch.optim.Adagrad.step:1 torch.optim.Adam.step:1 torch.optim.Adamax.step:1
#: torch.optim.LBFGS.step:1 torch.optim.RMSprop.step:1 torch.optim.Rprop.step:1
#: torch.optim.SGD.step:1 torch.optim.SparseAdam.step:1
msgid "Performs a single optimization step."
msgstr ""

#: of torch.optim.ASGD.step:3 torch.optim.Adadelta.step:3
#: torch.optim.Adagrad.step:3 torch.optim.Adam.step:3 torch.optim.Adamax.step:3
#: torch.optim.LBFGS.step:3 torch.optim.RMSprop.step:3 torch.optim.Rprop.step:3
#: torch.optim.SGD.step:3 torch.optim.SparseAdam.step:3
msgid "A closure that reevaluates the model and returns the loss."
msgstr ""

#: of torch.optim.Adagrad:1
msgid "Implements Adagrad algorithm."
msgstr ""

#: of torch.optim.Adagrad:3
msgid ""
"It has been proposed in `Adaptive Subgradient Methods for Online Learning"
" and Stochastic Optimization`_."
msgstr ""

#: of torch.optim.ASGD:9 torch.optim.Adagrad:9 torch.optim.RMSprop:12
#: torch.optim.Rprop:6
msgid "learning rate (default: 1e-2)"
msgstr ""

#: of torch.optim.Adagrad:11
msgid "learning rate decay (default: 0)"
msgstr ""

#: of torch.optim.Adam:1
msgid "Implements Adam algorithm."
msgstr ""

#: of torch.optim.Adam:3
msgid "It has been proposed in `Adam: A Method for Stochastic Optimization`_."
msgstr ""

#: of torch.optim.Adam:8 torch.optim.SparseAdam:9
msgid "learning rate (default: 1e-3)"
msgstr ""

#: of torch.optim.Adam:10 torch.optim.SparseAdam:11
msgid ""
"coefficients used for computing running averages of gradient and its "
"square (default: (0.9, 0.999))"
msgstr ""

#: of torch.optim.Adam:13 torch.optim.Adamax:13 torch.optim.RMSprop:18
#: torch.optim.SparseAdam:14
msgid ""
"term added to the denominator to improve numerical stability (default: "
"1e-8)"
msgstr ""

#: of torch.optim.SparseAdam:1
msgid "Implements lazy version of Adam algorithm suitable for sparse tensors."
msgstr ""

#: of torch.optim.SparseAdam:3
msgid ""
"In this variant, only moments that show up in the gradient get updated, "
"and only those portions of the gradient get applied to the parameters."
msgstr ""

#: of torch.optim.Adamax:1
msgid "Implements Adamax algorithm (a variant of Adam based on infinity norm)."
msgstr ""

#: of torch.optim.Adamax:3
msgid "It has been proposed in `Adam: A Method for Stochastic Optimization`__."
msgstr ""

#: of torch.optim.Adamax:8
msgid "learning rate (default: 2e-3)"
msgstr ""

#: of torch.optim.Adamax:10
msgid ""
"coefficients used for computing running averages of gradient and its "
"square"
msgstr ""

#: of torch.optim.ASGD:1
msgid "Implements Averaged Stochastic Gradient Descent."
msgstr ""

#: of torch.optim.ASGD:3
msgid ""
"It has been proposed in `Acceleration of stochastic approximation by "
"averaging`_."
msgstr ""

#: of torch.optim.ASGD:11
msgid "decay term (default: 1e-4)"
msgstr ""

#: of torch.optim.ASGD:13
msgid "power for eta update (default: 0.75)"
msgstr ""

#: of torch.optim.ASGD:15
msgid "point at which to start averaging (default: 1e6)"
msgstr ""

#: of torch.optim.LBFGS:1
msgid "Implements L-BFGS algorithm."
msgstr ""

#: of torch.optim.LBFGS:4
msgid ""
"This optimizer doesn't support per-parameter options and parameter groups"
" (there can be only one)."
msgstr ""

#: of torch.optim.LBFGS:8
msgid ""
"Right now all parameters have to be on a single device. This will be "
"improved in the future."
msgstr ""

#: of torch.optim.LBFGS:12
msgid ""
"This is a very memory intensive optimizer (it requires additional "
"``param_bytes * (history_size + 1)`` bytes). If it doesn't fit in memory "
"try reducing the history size, or use a different algorithm."
msgstr ""

#: of torch.optim.LBFGS:16
msgid "learning rate (default: 1)"
msgstr ""

#: of torch.optim.LBFGS:18
msgid "maximal number of iterations per optimization step (default: 20)"
msgstr ""

#: of torch.optim.LBFGS:21
msgid ""
"maximal number of function evaluations per optimization step (default: "
"max_iter * 1.25)."
msgstr ""

#: of torch.optim.LBFGS:24
msgid "termination tolerance on first order optimality (default: 1e-5)."
msgstr ""

#: of torch.optim.LBFGS:27
msgid "termination tolerance on function value/parameter changes (default: 1e-9)."
msgstr ""

#: of torch.optim.LBFGS:30
msgid "update history size (default: 100)."
msgstr ""

#: of torch.optim.RMSprop:1
msgid "Implements RMSprop algorithm."
msgstr ""

#: of torch.optim.RMSprop:3
msgid ""
"Proposed by G. Hinton in his `course "
"<http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf>`_."
msgstr ""

#: of torch.optim.RMSprop:6
msgid ""
"The centered version first appears in `Generating Sequences With "
"Recurrent Neural Networks <https://arxiv.org/pdf/1308.0850v5.pdf>`_."
msgstr ""

#: of torch.optim.RMSprop:14 torch.optim.SGD:11
msgid "momentum factor (default: 0)"
msgstr ""

#: of torch.optim.RMSprop:16
msgid "smoothing constant (default: 0.99)"
msgstr ""

#: of torch.optim.RMSprop:21
msgid ""
"if ``True``, compute the centered RMSProp, the gradient is normalized by "
"an estimation of its variance"
msgstr ""

#: of torch.optim.Rprop:1
msgid "Implements the resilient backpropagation algorithm."
msgstr ""

#: of torch.optim.Rprop:8
msgid ""
"pair of (etaminus, etaplis), that are multiplicative increase and "
"decrease factors (default: (0.5, 1.2))"
msgstr ""

#: of torch.optim.Rprop:12
msgid "a pair of minimal and maximal allowed step sizes (default: (1e-6, 50))"
msgstr ""

#: of torch.optim.SGD:1
msgid "Implements stochastic gradient descent (optionally with momentum)."
msgstr ""

#: of torch.optim.SGD:3
msgid ""
"Nesterov momentum is based on the formula from `On the importance of "
"initialization and momentum in deep learning`__."
msgstr ""

#: of torch.optim.SGD:9
msgid "learning rate"
msgstr ""

#: of torch.optim.SGD:15
msgid "dampening for momentum (default: 0)"
msgstr ""

#: of torch.optim.SGD:17
msgid "enables Nesterov momentum (default: False)"
msgstr ""

#: of torch.optim.SGD:21 torch.optim.lr_scheduler.LambdaLR:14
#: torch.optim.lr_scheduler.MultiStepLR:16
#: torch.optim.lr_scheduler.ReduceLROnPlateau:45
#: torch.optim.lr_scheduler.StepLR:16
msgid "Example"
msgstr ""

#: of torch.optim.SGD:30
msgid ""
"The implementation of SGD with Momentum/Nesterov subtly differs from "
"Sutskever et. al. and implementations in some other frameworks."
msgstr ""

#: of torch.optim.SGD:33
msgid "Considering the specific case of Momentum, the update can be written as"
msgstr ""

#: of torch.optim.SGD:39
msgid ""
"where p, g, v and :math:`\\rho` denote the parameters, gradient, "
"velocity, and momentum respectively."
msgstr ""

#: of torch.optim.SGD:42
msgid ""
"This is in contrast to Sutskever et. al. and other frameworks which "
"employ an update of the form"
msgstr ""

#: of torch.optim.SGD:49
msgid "The Nesterov version is analogously modified."
msgstr ""

#: ../../source/optim.rst:130
msgid "How to adjust Learning Rate"
msgstr ""

#: ../../source/optim.rst:132
msgid ""
":mod:`torch.optim.lr_scheduler` provides several methods to adjust the "
"learning rate based on the number of epoches. "
":class:`torch.optim.lr_scheduler.ReduceLROnPlateau` allows dynamic "
"learning rate reducing based on some validation measurements."
msgstr ""

#: of torch.optim.lr_scheduler.LambdaLR:1
msgid ""
"Sets the learning rate of each parameter group to the initial lr times a "
"given function. When last_epoch=-1, sets initial lr as lr."
msgstr ""

#: of torch.optim.lr_scheduler.ExponentialLR:4
#: torch.optim.lr_scheduler.LambdaLR:4 torch.optim.lr_scheduler.MultiStepLR:5
#: torch.optim.lr_scheduler.ReduceLROnPlateau:7
#: torch.optim.lr_scheduler.StepLR:5
msgid "Wrapped optimizer."
msgstr ""

#: of torch.optim.lr_scheduler.LambdaLR:6
msgid ""
"A function which computes a multiplicative factor given an integer "
"parameter epoch, or a list of such functions, one for each group in "
"optimizer.param_groups."
msgstr ""

#: of torch.optim.lr_scheduler.ExponentialLR:8
#: torch.optim.lr_scheduler.LambdaLR:10 torch.optim.lr_scheduler.MultiStepLR:12
#: torch.optim.lr_scheduler.StepLR:12
msgid "The index of last epoch. Default: -1."
msgstr ""

#: of torch.optim.lr_scheduler.StepLR:1
msgid ""
"Sets the learning rate of each parameter group to the initial lr decayed "
"by gamma every step_size epochs. When last_epoch=-1, sets initial lr as "
"lr."
msgstr ""

#: of torch.optim.lr_scheduler.StepLR:7
msgid "Period of learning rate decay."
msgstr ""

#: of torch.optim.lr_scheduler.MultiStepLR:9 torch.optim.lr_scheduler.StepLR:9
msgid "Multiplicative factor of learning rate decay. Default: 0.1."
msgstr ""

#: of torch.optim.lr_scheduler.MultiStepLR:1
msgid ""
"Set the learning rate of each parameter group to the initial lr decayed "
"by gamma once the number of epoch reaches one of the milestones. When "
"last_epoch=-1, sets initial lr as lr."
msgstr ""

#: of torch.optim.lr_scheduler.MultiStepLR:7
msgid "List of epoch indices. Must be increasing."
msgstr ""

#: of torch.optim.lr_scheduler.ExponentialLR:1
msgid ""
"Set the learning rate of each parameter group to the initial lr decayed "
"by gamma every epoch. When last_epoch=-1, sets initial lr as lr."
msgstr ""

#: of torch.optim.lr_scheduler.ExponentialLR:6
msgid "Multiplicative factor of learning rate decay."
msgstr ""

#: of torch.optim.lr_scheduler.ReduceLROnPlateau:1
msgid ""
"Reduce learning rate when a metric has stopped improving. Models often "
"benefit from reducing the learning rate by a factor of 2-10 once learning"
" stagnates. This scheduler reads a metrics quantity and if no improvement"
" is seen for a 'patience' number of epochs, the learning rate is reduced."
msgstr ""

#: of torch.optim.lr_scheduler.ReduceLROnPlateau:9
msgid ""
"One of `min`, `max`. In `min` mode, lr will be reduced when the quantity "
"monitored has stopped decreasing; in `max` mode it will be reduced when "
"the quantity monitored has stopped increasing. Default: 'min'."
msgstr ""

#: of torch.optim.lr_scheduler.ReduceLROnPlateau:14
msgid ""
"Factor by which the learning rate will be reduced. new_lr = lr * factor. "
"Default: 0.1."
msgstr ""

#: of torch.optim.lr_scheduler.ReduceLROnPlateau:17
msgid ""
"Number of epochs with no improvement after which learning rate will be "
"reduced. Default: 10."
msgstr ""

#: of torch.optim.lr_scheduler.ReduceLROnPlateau:20
msgid ""
"If ``True``, prints a message to stdout for each update. Default: "
"``False``."
msgstr ""

#: of torch.optim.lr_scheduler.ReduceLROnPlateau:23
msgid ""
"Threshold for measuring the new optimum, to only focus on significant "
"changes. Default: 1e-4."
msgstr ""

#: of torch.optim.lr_scheduler.ReduceLROnPlateau:26
msgid ""
"One of `rel`, `abs`. In `rel` mode, dynamic_threshold = best * ( 1 + "
"threshold ) in 'max' mode or best * ( 1 - threshold ) in `min` mode. In "
"`abs` mode, dynamic_threshold = best + threshold in `max` mode or best - "
"threshold in `min` mode. Default: 'rel'."
msgstr ""

#: of torch.optim.lr_scheduler.ReduceLROnPlateau:32
msgid ""
"Number of epochs to wait before resuming normal operation after lr has "
"been reduced. Default: 0."
msgstr ""

#: of torch.optim.lr_scheduler.ReduceLROnPlateau:35
msgid ""
"A scalar or a list of scalars. A lower bound on the learning rate of all "
"param groups or each group respectively. Default: 0."
msgstr ""

#: of torch.optim.lr_scheduler.ReduceLROnPlateau:39
msgid ""
"Minimal decay applied to lr. If the difference between new and old lr is "
"smaller than eps, the update is ignored. Default: 1e-8."
msgstr ""

